\chapter{sheet-5 : Eigenvalue and Eigenvectors of Operators}
%% writing by shahnoor
\ifpdf
\graphicspath{{Chapter5/figs/}}
\else
\graphicspath{{Chapter5/figs/}}
\fi


The ket $\ket{\alpha}$ is called the eigenvector or eigenket of the operator $A$ if
\begin{equation}
	A\ket{\alpha} = \alpha \ket{\alpha}
\end{equation}
The number $\alpha$ is called the eigenvalue. Thus the effect of $\hat{A}$ on an eigenket of $A$ is merely multiplication by a number.

\section{Eigenvalues and Eigenvectors of a Hermitian Operator}
We now take up the eigenvalue problem of a hermitian operators. Two theorems are of vital important in this content.
\begin{theorem}
	The eigenvalues of a hermitian operator are real.
\end{theorem}

\begin{theorem}
	The eigenvectors of a hermitian operator belonging to different eigenvalues are orthogonal.
\end{theorem}

\begin{proof}
	Let $A$ be a hermitian operator and 
	\begin{eqnarray}
		\label{chapter5.eqn1}
		A\ket{\alpha_1} &= \alpha_1 \ket{\alpha_1}\\
		\label{chapter5.eqn2}
		A\ket{\alpha_2} &= \alpha_2 \ket{\alpha_2}
	\end{eqnarray}
	From equation (\ref{chapter5.eqn1}) we have,
	\begin{equation}
		\bra{\alpha_2}A\ket{\alpha_1} = \alpha_1 \braket{\alpha_2}{\alpha_1}
		\label{chapter5.eqn3}
	\end{equation}
	Next we take the adjoint of equation (\ref{chapter5.eqn2})
	\begin{equation}
		\bra{\alpha_2}A^{\dagger} = \alpha_2^* \bra{\alpha_2}
	\end{equation}
	Since $A$ is hermitian, i.e., $A^\dagger = A$, we get
	\begin{equation}
		\bra{\alpha_2}A = \alpha_2^* \bra{\alpha_2}
	\end{equation}
	Hence
	\begin{equation}
		\bra{\alpha_2}A\ket{\alpha_1} = \alpha_2^* \braket{\alpha_2}{\alpha_1}
		\label{chapter5.eqn4}
	\end{equation}
	Combining equation (\ref{chapter5.eqn3}) and (\ref{chapter5.eqn4}) we get
	\begin{equation}
		(\alpha_1 - \alpha_2^*) \braket{\alpha_2}{\alpha_1} = 0
		\label{chapter5.eqn5}
	\end{equation}
	If we let $\alpha_2 = \alpha_1$ and recaling that $\braket{\alpha_1}{\alpha_1} \neq 0$, it follows that
	\begin{equation}
		\alpha_1 - \alpha_1^* = 0
	\end{equation}
	i.e., $\alpha_1$ is real. Since eigenvalues are proved to be real, we can write equation (\ref{chapter5.eqn5}) as
	\begin{equation}
		(\alpha_1 - \alpha_2) \braket{\alpha_2}{\alpha_1} = 0
		\label{chapter5.eqn6}
	\end{equation}
	If $\alpha_1 \neq \alpha_2$, we must have
	\begin{equation}
		\braket{\alpha_2}{\alpha_1} = 0
	\end{equation}
	i.e., eigenvectors belongings to differnt eigenvalues are orthogonal. Owing to the linearity of the operators $\hat{A}$ we can normalize the eigenvectors. We shall therefore usually assume that 
	\begin{equation}
		\braket{\alpha_1}{\alpha_2} = \delta_{\alpha_1\alpha_2}
	\end{equation}
	Thus, the eigenvectors of a hermitian operator form an orthonormal (and hence linearly independent vectors), i.e.,
	\begin{equation}
		\braket{\alpha_i}{\alpha_j} = \delta_{\alpha_i\alpha_j}
	\end{equation}
\end{proof}


\section{Determination of eigenvalues and eigenvectors of a Hermitian Operator}
	Let $A$ be a hermitian operator. Consider the eigenvalue equation
	\begin{equation}
		A \ket{\lambda} = \lambda \ket{\lambda}
		\label{chapter5.eqn7-eigenvalue-hermitian}
	\end{equation}
	To find the eigenvalue and the corresponding eigenvectors, we have to choose a basis in the vector space and convert the operator equation (\ref{chapter5.eqn7-eigenvalue-hermitian}) into a matrix equation. For simplicity, we will assume that the vector space is finite dimensional with dimension $n$. \\
	Now choosing an orthonormal basis set $\{\ket{u_i}\}$, we can cast equation (\ref{chapter5.eqn7-eigenvalue-hermitian}) as a matrix equation of the following form:
	\begin{equation}
		\left[
		\begin{matrix}
			A_{11} & A_{12} & \ldots & A_{1n} \\
			A_{21} & A_{22} & \ldots & A_{2n} \\
			\vdots & \vdots & \vdots \vdots \vdots & \vdots \\
			A_{n1} & A_{n2} & \ldots & A_{nn} \\
		\end{matrix}
		\right]\left[
		\begin{matrix}
			x_1 \\ x_2 \\ \vdots \\ x_n
		\end{matrix}
		\right]
		=
		\lambda
		\left[
		\begin{matrix}
			x_1 \\ x_2 \\ \vdots \\ x_n
		\end{matrix}
		\right]
		\label{chapter5.eqn7-eigenvalue-matrix-form}
	\end{equation}
	Here $x_1, x_2, \ldots, x_n$ are the components of the eigenvector $\ket{\lambda}$ in \textbf{directions} $\ket{u_1}, \ket{u_2}, \ldots, \ket{u_n}$ respectively, i.e.,
	\begin{equation}
		x_i \equiv \braket{u_i}{\lambda} \text{ ; } i=1,2,\ldots,n
	\end{equation}
	Equation (\ref{chapter5.eqn7-eigenvalue-matrix-form} is a set of linear homogeneous equations which possess non-trivial solutions only if)

\begin{equation}
	\left|
	\begin{matrix}
		(A_{11}-\lambda) & A_{12} & \ldots & A_{1n} \\
		A_{21} & (A_{22}-\lambda) & \ldots & A_{2n} \\
		\vdots & \vdots & \vdots \vdots \vdots & \vdots \\
		A_{n1} & A_{n2} & \ldots & (A_{nn}-\lambda) \\
	\end{matrix}
	\right| = 0
\end{equation}
	or in short
	\begin{equation}
		\det(A_{ij} - \lambda \delta_{ij}) = 0
	\end{equation}
	In matrix notation, we can write
	\begin{equation}
		\left|\mathbb{A} - \lambda \mathbb{1} \right| = 0
	\end{equation}
	This equation, which is a polynomial equation of degree $n$ in the unknown $\lambda$, is called the secular equation of the matrix $\mathbb{A}$. Solving this equation we get $n$ roots which we label as 
	\begin{equation}
		\lambda_1, \lambda_2, \ldots, \lambda_n \nonumber
	\end{equation}
	Now, we can distinguish two cases. If the $n$ eigenvalues are all distinct, we say that the eigenvalues are \textit{non-degenerate}. However, it may so happen that some of the eigenvalues are repeated. Those eigenvalues which are repeated are called \textit{degenerate}
	eigenvalues and the number of times an eigenvalue is repeated is called the \textit{order of degeneracy} of that eigenvalue.
	
	\section{Non-degenerate roots}
	In this case all the roots $\lambda_i$ are distinct and there are $n$ of them if the vector space is $n$ dimensional. If $A$ is hermitian, the roots are real. For a non-hermitian operator some or all of the roots may be complex.
	
	
	Now, for each eigenvalue (root of secular equation) we can solve the eigenvalue equation (\ref{chapter5.eqn7-eigenvalue-matrix-form}) to get $n$ linearly independent eigenvectors $\ket{\lambda_i}$. Since the $\ket{\lambda_i}$'s are linearly independent, they span the $n$ dimensional vector space, i.e., they form a complete set of basis vectors.
	
	
	
	If $A$ is hermitian, the eigenvectors are guarenteed to b orthogonal, i.e., $\braket{\lambda_i}{\lambda_j} = 0$ if $i\neq j$. However, for a non-hermitian operator the eigenvectors may or may not be orthogonal.
	Using the eigenvectors of $A$ as the basis (This basis is called the eigenbasis of $A$), the matrix representation of $A$ is
	\begin{equation}
		A_{ij}^\prime \equiv \bra{\lambda_i} A \ket{\lambda_j} = \lambda_j \braket{\lambda_i}{\lambda_j}
		\label{chapter5.eqn8-matrix-reps}
	\end{equation}
	For hermitian $A$, we always have $\braket{\lambda_i}{\lambda_j} = 0$ if $i \neq j$, and , further we can normalize each eigenvector $\ket{\lambda_i}$. Thus, for a hermitian operator, the eigenbasis is an orthogonal set, i.e., 
	\begin{equation}
		\braket{\lambda_i}{\lambda_j} = \delta_{ij}
		\label{chapter5.eqn9-eigenbasis}
	\end{equation}
	Therefore, the matrix representation of the oprator $A$ in its eigenbasis is diagonal, i.e.,
	\begin{equation}
		A_{ij}^\prime = \lambda_j\delta_{ij}
		\label{chapter5.eqn9-matrix-reps-diagonal}
	\end{equation}
	Writing out the matrix ($A_{ij}^\prime$) in full
	\begin{equation}
		\mathbb{A}^\prime = \left[
		\begin{matrix}
			\lambda_1 &	0	& 0	& \ldots	& 0 \\			
			0 &	\lambda_2	& 0	& \ldots	& 0 \\
			\vdots &	\vdots	& \vdots	& \vdots \quad \vdots	& \vdots \\	
			0 & 0	& 	 0 & \ldots	& \lambda_n  \\
		\end{matrix}
		\right]
	\end{equation}
	An operator or a matrix $\mathbb{A}$ is said to be diagonalizable, if we can find a basis in which the matrix becomes diagonal. For a hermitian operator we can always find a basis, the eigenbasis of the operator, in which the matrix representation of the operator is diagonal with the eigenvalues as the diagonal elements.
	
	
	
	For a non-hermitian operator in an $n$  dimensional vector space, there is no guarantee that the matrix reprsentation $A_{ij}^\prime$ in the eigenbasis of the operator is diagonal. This is because, in general, the eigenvectors are  not orthogonal, i.e., $\braket{\lambda_i}{\lambda_j} \neq \delta_{ij}$
	
	\section{Degenerate roots}
	The secular equation (\ref{chapter5.eqn8-matrix-reps}) may have roots some or all of which are repeated. So, the number of \underline{distinct} eigenvalues is now less than the dimension of the vector space.
	
	
	As an example, suppose we have a six-dimensional vector space ($n=6$) with three distinct roots $\lambda_1, \lambda_2, \lambda_3$. Suppose $\lambda_1$ is repeated three times, $\lambda_2$ is repeated two times and $\lambda_3$ occurs only once. Thus the six roots of the secular equation are $\lambda_1, \lambda_1, \lambda_1, \lambda_2, \lambda_2, \lambda_3$.
	
	
	
	
	
	We say $\lambda_1$ is three-fold degenerate, $\lambda_2$ is two-fold degenerate and $\lambda_3$ is non-degenerate. We represent the order of degeneracy of a distinct eigenvalue $\lambda_i$ by $g_{\lambda_i}$. In the present example, $g_{\lambda_1}=3, g_{\lambda_2}=2$ and $ g_{\lambda_3}=1$. We have
	\begin{equation}
		g_{\lambda_1} + g_{\lambda_2} + g_{\lambda_3} = 6 \quad \text{dimension of the vector space}
	\end{equation}
	Now, it may be shown that, for a \underline{hermitian operator} if a root $\lambda$ is $g$-fold degenerate, there are always $g$ linearly independent eigenvectors corresponding to $\lambda$. For a non-hermitian operator, there may not exist as many linearly independent eigenvectors as the order of degeneracy.
	
	
	
	In the above example, if $\lambda_1, \lambda_2$ and $\lambda_3$ are eigenvalues of a hermitian operator, there are three linearly independent eigenvectors with eigenvalue $\lambda_1$, two linearly independent eigenvectors with eigenvalue $\lambda_2$ and one eigenvector with eigenvalue $\lambda_3$. Thus, the total number of linearly indepent eigenvector is six, the same as the dimension of the vector space. Hence these six linearly independent eigenvectors form a \textit{complete basis set of vectors}.
	
	
	
	If, however, $\lambda_1, \lambda_2$ and $\lambda_3$ are eigenvalues of a non-hermitian operator with the same eigenvalues, there may not exist three linearly independent eigenvectors with eigenvalue $\lambda_1$, or  two linearly independent eigenvectors with eigenvalue $\lambda_2$. In such a situation, the number of linearly independent eigenvectors of the non-hermitian operator $A$ is less than the dimension $n$ of the vector space. Hence, these eigenvectors \textit{do not form a basis set} for a $n$ dimensional vector space.
	
	
	
	\section{Digonalization of a Hermitian Operator}
	Let $A$ be a hermitian operator with distinct eigenvalues $\lambda_1,\lambda_2\ldots$. Some or all of the eigenvalues may be degenerate, with the order or degree of degeneracy of an eigenvalue $\lambda_i$ being denoted by $g_{\lambda_i}$. If $g_{\lambda_j}=1$ for some $\lambda_j$, then $\lambda_j$ is said to be non-degenerate.
	
	
	Since $A$ is hermitian there will always be $g_{\lambda_i}$ linearly independent eigenvectors, each belonging to the same eigenvalue $\lambda_i$. We will now require another index, $s^{(i)}$, to distinguish between these linearly independent eigenvectors. We write
	\begin{equation}
		A \ket{\lambda_i, s^{(i)}} = \lambda_i \ket{\lambda_i, s^{(i)}}
		\label{chapter5.eqn9-eigenvectors-with-two-index}
	\end{equation}
	where $s^{(i)} = 1, 2, \ldots, g_{\lambda_i}$. A linear combination of the degenerate eigenvectors is also an eigenvector with the same eigenvalue $\lambda_i$. So we have
	\begin{equation}
		A \left(\sum_{s^{(i)}}^{g_{\lambda_i}}  C_{s^{(i)}} \ket{\lambda_i, s^{(i)}}\right) = \lambda_i \left(\sum_{s^{(i)}}^{g_{\lambda_i}}  C_{s^{(i)}} \ket{\lambda_i, s^{(i)}}\right)
		\label{chapter5.eqn10-combination-degenerate-eigenvectors}
	\end{equation}
	Thus, the set of vectors $\{ \ket{\lambda_i, s^{(i)}}; \quad \lambda_i \quad \text{fixed}, \quad s^{(i)}=1,2\ldots,g_{\lambda_i} \}$ spans a subspace, called the eigen subspace of $\lambda_i$, of the original $n$ dimensional vector space. The eigenvectors belonging to a degenerate eigenvalue need not be orthogonal to each other even if they are linearly independent, as the general theorem of hermitian operators proves the orthogonality of eigenvectors belonging to different eigenvalues.
	
	
	
	However, using Schmidt orthonormalization procedure (see section (\ref{chapter2.schmidt-orthonormalization-method})), we can get a set of $g_{\lambda_i}$ orthogonal 
	eigenfunctions %@@@@@@@ or
%	 eigenvectors %@@@@@@@ ?
	  of eigenvalue $\lambda_i$ from a set of $g_{\lambda_i}$ linearly independent set of
	  eigenfunctions
%	 eigenvectors %@@@@@@@ ?
	   of eigenvalue $\lambda_i$.
	   
	   
	   Thus, all the eigenvectors of the hermitian operator, wherether belonging to same or different eigenvalues can be considered as orthogonal to each other. Further, they are also normalized. Using the set of orthonormal eigenfunctions as the basis, the matrix representation of $A$ is diagonal.
	   
	   
	   
	   As a concrete example of diagonalization of a hermitian operator, suppose we have a finite seven-dimensional linear vector space. If, all the eigenvectors are non-degenerate, then there are seven distinct eigenvalues $\lambda_1, \ldots, \lambda_7$ and corresponding to each eigenvalue there will be one eigenvector $\ket{\lambda_1}, \ldots, \ket{\lambda_7}$. These eigenvectors are orthogonal and they are normalized. Using the eigenvectors as the basis, the matrix representation of $A$ is 
	   \begin{equation}
		   A = \left[
		   \begin{matrix}
	\lambda_1 & 0 & 0 & 0 & 0 & 0 & 0 \\
	0 & \lambda_2 & 0 & 0 & 0 & 0 & 0 \\
	0 & 0 & \lambda_3 & 0 & 0 & 0 & 0 \\
	0 & 0 & 0 & \lambda_4 & 0 & 0 & 0 \\
	0 & 0 & 0 & 0 & \lambda_5 & 0 & 0 \\
	0 & 0 & 0 & 0 & 0 & \lambda_6 & 0 \\
	0 & 0 & 0 & 0 & 0 & 0 & \lambda_7 \\
		   \end{matrix}
		   \right]
	   \end{equation}
	But if some of the eigenvalues are degenerate, then the number of distinct eigenvalues will be less than seven. Suppose that there are three distinct eigenvalues $\lambda_1, \lambda_2, \lambda_3$. Also suppose that $\lambda_1$ is three-fold degenerate and $\lambda_2$ and $\lambda_3$ both are two-fold degenerate. Thus $g_{\lambda_1}=3, g_{\lambda_2}=2, g_{\lambda_3}=2$ and $g_{\lambda_1} + g_{\lambda_2} + g_{\lambda_3}=7$ which is the dimension of the vector space.
	
	
	
	There are three linearly independent (but not necessarily orthogonal) eigenvectors with eigenvalue $\lambda_1$ and two linearly independent eigenvectors for each eigenvalue $\lambda_2$ and $\lambda_3$. The eigenvectors with eigenvalue $\lambda_1$ can be labled as 
	$\ket{\lambda_1, s^{(1)}}$ with $s^{(1)} = 1,2,3$, i.e., $\ket{\lambda_1,1}, \ket{\lambda_1,2}, \ket{\lambda_1,3}$.
	
	These three eigenvectors span a subspace of the original seven-dimensional vector space $H$. The subspace is called the eigensubspace of $\lambda_1$ and is denoted by $H_{\lambda_1}$ or simply $H_1$. The eigenvectors belonging to $\lambda_2$ and $\lambda_3$ are labeled similarly. Then two linearly independent eigenvectors with eigenvalue $\lambda_2$ span a two-dimensional subspace $H_2$ and the two linearly independent vectors belonging to $\lambda_3$ span the eigensubspace $H_3$. These three subspaces make up the full vector space $H$. We write
	\begin{equation}
		H = H_1 \bigoplus H_2 \bigoplus H_3
	\end{equation}
	The seven linearly independent eigenvectors $\{\ket{\lambda_i, s^{(i)}}, \quad s^{(i)}=1,2,\ldots g_{\lambda_i}, \quad i=1,2,3 \}$
	can now be used as a basis to find the matrix representation of $A$. If the basis vectors within an eigensubspace are not made orthogonal, the matrix representation of $A$ is block-diagonal as shown below.
	\begin{equation}
		\left[\begin{array}{@{}c|c@{}|c@{}}
		\begin{matrix}
		a_{11} & a_{12} & a_{13} \\
		a_{21} & a_{22} & a_{23} \\
		a_{31} & a_{32} & a_{33}
		\end{matrix}
		& \bigzero & \bigzero\\
		\hline
		\bigzero &
		\begin{matrix}
		b_{11} & b_{12} \\
		b_{21} & b_{22}
		\end{matrix}
		& \bigzero \\
		\hline
		\bigzero & \bigzero &
		\begin{matrix}
		c_{11} & c_{12} \\
		c_{21} & c_{22}
		\end{matrix}
		\end{array}\right]
	\end{equation}
	
%	\[
%	\begin{array}{l@{{}={}}c}
%	\text{Mat}_{\varphi\text{ to }M} & \left(\begin{array}{@{}ccccc@{}}
%	1 & 1 & 1 & 1 & 1 \\
%	0 & 1 & 0 & 0 & 1 \\
%	0 & 0 & 1 & 0 & 1 \\
%	0 & 0 & 0 & 1 & 1 \\
%	0 & 0 & 0 & 0 & 1
%	\end{array}\right)
%	\end{array}
%	\]
%	
%
%	\[
%	\text{Mat}_{\varphi\text{ to }M} = \kbordermatrix{
%		& c_1 & c_2 & c_3 & c_4 & c_5 \\
%		r_1 & 1 & 1 & 1 & 1 & 1 \\
%		r_2 & 0 & 1 & 0 & 0 & 1 \\
%		r_3 & 0 & 0 & 1 & 0 & 1 \\
%		r_4 & 0 & 0 & 0 & 1 & 1 \\
%		r_5 & 0 & 0 & 0 & 0 & 1
%	}
%	\]
	
	Writing with basis
	\begin{equation}
	\begin{blockarray}{cccccccc}
	\ & 1 & 2 & 3 & 4 & 5 & 6 & 7 \\
	\ & \ket{\lambda_1, 1} & \ket{\lambda_1, 2} & \ket{\lambda_1, 3} & \ket{\lambda_2, 1} & \ket{\lambda_2, 2} & \ket{\lambda_3, 1} & \ket{\lambda_3, 2} \\
		\begin{block}{c[ccccccc]}
		\bra{\lambda_1, 1} & a_{11} & a_{12} & a_{13} & 0 & 0 & 0 & 0 \\
		\bra{\lambda_1, 2} & a_{21} & a_{22} & a_{23} & 0 & 0 & 0 & 0 \\
		\bra{\lambda_1, 3} & a_{31} & a_{32} & a_{33} & 0 & 0 & 0 & 0 \\ 
		\bra{\lambda_2, 1} & 0 & 0 & 0 & b_{11} & b_{12} & 0 & 0 \\
		\bra{\lambda_2, 2} & 0 & 0 & 0 & b_{21} & b_{22} & 0 & 0 \\ 
		\bra{\lambda_3, 1} & 0 & 0 & 0 & 0 & 0 & c_{21} & c_{22}\\
		\bra{\lambda_3, 2} & 0 & 0 & 0 & 0 & 0 & c_{21} & c_{22}  \\ 
		\end{block}
	\end{blockarray}
	\end{equation}
	Each non-zero block is a square matrix. The first block is a $3\times 3$ matrix, the second one is a $2\times 2$ matrix and the rhird one is a $2\times 2$ matrix. These blocks themselves are not diagonal if the basis vectors of the three eigensubspaces are not orthogonalized. If we orthogonalize the basis vector in each eigensubspace, then each block will also be diagonal. The matrix representation of $A$ will then be
	
	\begin{equation}
	\begin{blockarray}{cccccccc}
	\ & \ket{\lambda_1, 1} & \ket{\lambda_1, 2} & \ket{\lambda_1, 3} & \ket{\lambda_2, 1} & \ket{\lambda_2, 2} & \ket{\lambda_3, 1} & \ket{\lambda_3, 2} \\
	\begin{block}{c[ccccccc]}
	\bra{\lambda_1, 1} & \lambda_1 & 0 & 0 & 0 & 0 & 0 & 0 \\
	\bra{\lambda_1, 2} & 0 & \lambda_1 & 0 & 0 & 0 & 0 & 0 \\
	\bra{\lambda_1, 3} & 0 & 0 & \lambda_1 & 0 & 0 & 0 & 0 \\ 
	\bra{\lambda_2, 1} & 0 & 0 & 0 & \lambda_2 & 0 & 0 & 0 \\
	\bra{\lambda_2, 2} & 0 & 0 & 0 & 0 & \lambda_2 & 0 & 0 \\ 
	\bra{\lambda_3, 1} & 0 & 0 & 0 & 0 & 0 & \lambda_3 & 0\\
	\bra{\lambda_3, 2} & 0 & 0 & 0 & 0 & 0 & 0 & \lambda_3  \\ 
	\end{block}
	\end{blockarray}
	\end{equation}
	Thus the matrix representation of a hermitian operator $A$ is diagonalized.
	
	
	We have proved that a hermitian operator (or a hermitian matrix) is always diagonalizable in a finite dimensional vector space. By diagonalizable we mean that we can always find a basis in which the matrix representation of $A$ is diagonal. This basis is simply the basis consisting of the orthogonalized eigenvectors of $A$, called eigenbasis of $A$.
	
	
	 The eigenvectors of a non-hermitian operator my be fewer in numbers than the dimension of the vector space if there is degeneracy. If an  eigenvector $\lambda_i$ is $g_i$-fold degenerate, then the number of linearly independent eigenvectors belonging to $\lambda_i$ may be less than $g_i$. Therefore, the eigenvectors of a non-hermitian operator cannot form a basis set for the vector space. Therefore, a non-hermitian operator is not diagonalizable.
	
	
	\section{Basis independence of the eigenvalues of an Operator}
	Basis independence refers to representation independence. To find the eigenvalues of a hermitian operator $\hat{A}$, first we choose an orthonormal basis set $\{\ket{u_i}\}$ and form the matrix representation of the operator. Then we solve the secular equation to find the eigenvalues. Although we have to integrate a basis set to find the eigenvalues, it is easy to verify that the eigenvalues are independent of the choice of the basis. 
	
	
	Indeed, if we choose a new orthogonal set of basis vectors $\{\ket{u_i}\}$ which are related to the old set according to 
	\begin{equation}
		\ket{u_i^\prime} = \sum_j \ket{u_j}\braket{u_j}{u_i^\prime}
	\end{equation}
	Then the new matrix representation of the operator $A$ is related to the old representation by a similarigy transformation with a \textit{unitary} matrix. This is easy to see
	\begin{eqnarray}
		A_{ij}^\prime 
		&\equiv \bra{u_i\prime} A \ket{u_j^\prime} \nonumber\\
		&= \sum_{jk} \braket{u_i^\prime}{u_j}\bra{u_j}A\ket{u_k}\braket{u_k}{u_j^\prime} \nonumber\\
		&= \sum_{jk} S_{ij} A_{jk} S_{jk}^* \nonumber\\
		&= \sum_{jk} S_{ij} A_{jk} S_{jk}^\dagger
		\label{chapter5.eqn2-similarity-trans}
	\end{eqnarray}
	Where we have defined the matrix $S$ as
	\begin{equation}
		S_{ij} \equiv \braket{u_i^\prime}{u_j}
	\end{equation}
	The matrix $S$ is unitary as shown precisely. In matrix notation, we write equation (\ref{chapter5.eqn2-similarity-trans}) as,
	\begin{equation}
		A^\prime = S A S^\dagger = S A S^{-1}
	\end{equation}
	since $S$ is unitary matrix. Then
	\begin{eqnarray}
		\det(\mathbb{A}^\prime - \lambda \mathbb{I}) 
		&= \det(\mathbb{S}\mathbb{A}^\prime \mathbb{S}^{-1} - \lambda \mathbb{S} \mathbb{I} \mathbb{S}^{-1}) \nonumber \\
		&= \det(\mathbb{S}(\mathbb{A} - \lambda \mathbb{I}) \mathbb{S}^{-1})\nonumber \\
		&= \det(\mathbb{A} - \lambda \mathbb{I})
	\end{eqnarray}
	Thus, there is no change in the secular equation even if we change the basis set. Since the eigenvalues are the roots of the secular equation, the eigenvalues are representation independent. They are characteristics of the operator $\hat{A}$ itself, and not of any particular representation.
	
	

	Next, we will show that the determinant and the trace of the matrix representation $A$ are independent of the basis used for the representation.
	
	Since $\mathbb{A}^\prime = \mathbb{S} \mathbb{A} \mathbb{S}^{-1}$ we have 
	\begin{eqnarray}
		\det(\mathbb{A}^\prime) 
		&= \det(\mathbb{S} \mathbb{A} \mathbb{S}^{-1}) \nonumber \\
		&= \det(\mathbb{S}^{-1}\mathbb{S} \mathbb{A} ) \nonumber \\
		&= \det(\mathbb{A})
	\end{eqnarray}
	i.e., the determinant is independent of the representation.
	
	We also have
	\begin{equation}
		\Tr (\mathbb{A}^\prime) = \Tr (\mathbb{S}\mathbb{A}^\prime\mathbb{S}^{-1})=\Tr(\mathbb{S}^{-1}\mathbb{S} \mathbb{A}) = \Tr (\mathbb{A})
	\end{equation}
	i.e., the trace is also independent of the representation. In the above derivations, we have used the identities,
	\begin{eqnarray}
		\det( \mathbb{A}\mathbb{B}) = \det(\mathbb{B}\mathbb{A}) \\
		\Tr( \mathbb{A}\mathbb{B}) = \Tr(\mathbb{B}\mathbb{A})
	\end{eqnarray}
	where $\mathbb{A}$ and $\mathbb{B}$ are square matrices. Now, if we use the eigenbasis of the hermitian operator $A$ for the representation, then $\mathbb{A}$ is a diagonal matrix with
	\begin{eqnarray}
		\det \mathbb{A} &= \prod_i \lambda_i \\
		\Tr \mathbb{A} &= \sum_i \lambda_i 
	\end{eqnarray}
	If an eigenvalue is $g$-fold degenerate, then that eigenvalue has to be repeated $g$ times while calculating the determinant and trace of the matrix $\mathbb{A}$.
	
	
	
	\section{Infinite dimensional vector space}
	We have shown that a linear operaor is a finite $n$-dimensional vector space has $n$ eigenvalues some of which may be repeated. If the operator is hermitian, then the eigenvalues are real and eigenvectors belonging to different eigenvalues are orthogonal and hence linearly independent.
	
	Further, if an eigenvalue $\lambda$ of a hermitian operator is $g$-fold degenerate, then there are $g$ linearly independent eigenvectors corresponding to $\lambda$, these degenerate eigenvectors are not necessarily orthogonal even if they are linearly independent. However, we can orthogonalize the degenerate eigenvectors using the Schmidt orthonormalization procedure (see section (\ref{chapter2.schmidt-orthonormalization-method})). 
	
	
	Thus in a finite $n$-dimensional vector space, the eigenvectors of any hermitian operator form a set of orthonormal basis vectors.
	
	
	In an infinite dimensional vector space, the number of eigenvalues and eigenvectors of a hermitian operators are infinitely many. However, it is possible that the eigenvectors of some hermitian operators do not form a complete set in an infinite dimensional vector space.

	
	Hermitian operators are of vital importance in quantum mechanics because to every observable (e.g., position, linear momentum, angular momentum, spin etc.) we \textit{associate} a corresponding hermitian operator. Of course, there are hermitian operators which are not associated with any observable.
	
	
	The eigenvectors of a hermitian operator representing a physical observable form a complete set even in an infinite-dimensional Hilbert space. The eigenvectors of a hermitian operator not associated with any observale may not form a complete basis set in an infinite dimensional space.
	
	
	
	\section{Completeness condition for the eigenvectors of a Hermitian Operator}
	Let us assume that the eigenvalue spectrum of a hermitian operator $\hat{A}$ form a discrete set. In other words, the eigenvalues $a_i,\quad i=1,2,\ldots$ of the operator are discrete real numbers.
	
	Assume, for the time being, that the eigenvalues are non-degenerate so that these is only one linearly independent eigenvectors $\ket{a_i}$ corresponding to each eigenvalue $a_i$. The eigenvectors $\{\ket{a_i}, i=1,2,\ldots\}$ form a complete orthogonal set of basis vectors. Therefore, an arbitrary vector $\ket{\psi}$ of the vector space can be expanded as a linear combination of the vector in the basis set, i.e.,
	\begin{equation}
		\ket{\psi} = \sum_{i} a_i \ket{a_i}
	\end{equation}
	where $c_i = \braket{a_i}{\psi}$. Therefore, we can write
	\begin{equation}
		\ket{\psi} = \sum_i \braket{a_i}{\psi} \ket{a_i} = \sum_i \ket{a_i}\braket{a_i}{\psi}
	\end{equation}
	Since $\ket{\psi}$ is arbitrary, we must have
	\begin{equation}
		\hat{\mathbb{1}} = \sum_i \ket{a_i}\bra{a_i} = \sum_i \hat{P}_i
		\label{chapter5.eqn1-completeness}
	\end{equation}
	where
	\begin{equation}
		\hat{P}_i = \ket{a_i}\bra{a_i}
		\label{chapter5.eqn2-projection}
	\end{equation}
	is the projection along $\ket{a_i}$.
	
	Using the basis $\{\ket{u_i}\}$, any operator $\hat{O}$ can be expressed as 
	\begin{eqnarray}
		\hat{O} = \hat{\mathbb{1}} \hat{O} \hat{\mathbb{1}} 
		&= \sum_{i,j} \ket{a_i}\bra{a_i} \hat{O} \ket{a_j}\bra{a_j} \nonumber \\
		&= \sum_{i,j} \ket{a_i} O_{ij} \bra{a_j}
		\label{chapter5.eqn3-operator-as-matrix}
	\end{eqnarray}
	where $O_{ij} \equiv \bra{a_i} \hat{O} \ket{a_j}$ are the matrix element of  $\hat{O}$ in the basis $\{\ket{u_i}\}$. Since basis is the eigenbasis of the operator $\hat{A}$, the matrix elements of $\hat{A}$ in the basis will be diagonal, i.e., 
	\begin{equation}
		\hat{A} = \sum_i a_i \ket{a_i}\bra{a_i} = \sum_i a_i \hat{P}_i
		\label{chapter5.eqn4-diagonalized-operator-projection}
	\end{equation}
	Any other oberator $\hat{B}$ will in general not be diagonal in the eigenbasis of $\hat{A}$ unless the eigenvectors of $\hat{B}$ and $\hat{A}$ coincide. Later, we will see that two operators $\hat{A}$ and $\hat{B}$ have simultaneous eigenvectors if they commute, i.e., if $[\hat{A},\hat{B}]=0$.
	
	
	Now, we will generalize the notation to include degeneracy, suppose the eigenvalue $a_i$ is a $g_i$ fold degenerate. Then the eigenvectors belonging to the eigenvalue $a_i$ is written as $\ket{a_i, s^{(i)}}$ where $s^{(i)}$ can take values $1,2,\ldots, g_i$. The set of vectors $\{\ket{a_i, s^{(i)}}, \quad s^{(i)}=1,2,\ldots,g_i;\quad i=1,2,\ldots\}$ Form a complete orthogonal set. The completeness condition is
	\begin{equation}
		\sum_{i=1}^{\infty} \sum_{s^{i}=1}^{g_i} \ket{a_i, s^{(i)}}\bra{a_i, s^{(i)}} = \hat{\mathbb{1}}
		\label{chapter5.eqn5-completeness-degenerate}
	\end{equation}
	and the orthogonality condition is
	\begin{equation}
		\braket{a_i, s^{(i)}}{a_j, s^{(j)}} = \delta_{ij}\delta_{s^{i}s^{(j)}}
		\label{chapter5.eqn6-orthogonality-degenerate}
	\end{equation}
	We can rewrite equation (\ref{chapter5.eqn5-completeness-degenerate}) as (exactly as the non degenerate case)
	\begin{equation}
		\hat{\mathbb{1}} = \sum_i \hat{P}_i
		\label{chapter5.eqn7-completeness-degenerate-rewrite}
	\end{equation}
	where
	\begin{equation}
		\hat{P}_i = \sum_{s^{(i)}=1}^{g_i} \ket{a_i, s^{(i)}}\bra{a_j, s^{(j)}}
		\label{chapter5.eqn8-projection-degenerate}
	\end{equation}
	is the projection operator on the eigensubspace of $a_i$. The operator $\hat{A}$ can then be written in its own eigenbasis as 
	$\hat{A} = \sum_i a_i \hat{P}_i$ with $\hat{P}_i$ given in equation (\ref{chapter5.eqn8-projection-degenerate})
	
	
	
	\section{Hermitian operator with continuous eigenvalue spectrum}
	In Quantum Mechanics we encounter hermitian operator like position operator, momentum operator whose eigenvalues range over a continuum of real values. Such an eigenvalue spectrum is called continuous. There are also hermitian operators whose eigenvalue spectrum may be both discrete and continuous.
	
		\subsection{Continuous Spectrum}
		Let us consider an operator $A$ whose eigenvalues can vary continuously over a certain domain of real numbers
		\begin{equation}
			A \ket{a} = a \ket{a}
			\label{chapter5.eqn9-eigenvalue}
		\end{equation}
		If there is degeneracy, we will put in a second index $s$ to distinguish between degenerate vectors. Thus we may write $\ket{a\quad s}$ to denote a degenerate eigenvector. We assume that there is no degeneracy. In case of degeneracy it is a simple matter to generalize our notations. We assume that the vectors $\ket{a}$ form a complete set. The completeness condition can be written as
		\begin{equation}
			\int da \quad \ket{a}\bra{a} = \hat{\mathbb{1}}
			\label{chapter5.eqn10-completenss-continuous}
		\end{equation}
		Where the integral extends over the entire domain in which $a$ varies. Usually this domain is $-\infty$ to $\infty$.
		
		Two eigenkets $\ket{a}$ and $\ket{a^\prime}$ with $a\neq a^\prime$ are orthogonal because $A$ is  hermitian operator, i.e.,
		\begin{equation}
			\braket{a}{a^\prime} = 0 ; \quad  a \neq a^\prime
			\label{chapter5.eqn11-orthogonal-continuous}
		\end{equation}
		What will the scalar product be if $a=a^\prime$ ? Can we take $\braket{a}{a} = 1$ as in the discrete case where we normalized the eigenkets as $\braket{a_i}{a_i} = 1$?
		
		
		The answer is \textbf{no}, i.e., in the case where the eigenvalues $a$ vary continuously, the kets $\ket{a}$ cannot be normalized to unity. To see this, expand an arbitrary ket $\ket{f}$ in the eigenbasis $\{\ket{a}\}$ of the operator $\hat{A}$. We have
		\begin{equation}
			\ket{f} = \int da^\prime \quad \ket{a^\prime}\braket{a^\prime}{f}
			\label{chapter5.eqn10-expansion-continuous}
		\end{equation}
		Taking the scalar product of $\ket{f}$ with $\ket{a}$, we get
		\begin{eqnarray}
		\braket{a}{f} 
		&= \int da^\prime \quad \ket{a^\prime}\braket{a^\prime}{f} \nonumber \\
		f(a) &= \int da^\prime \quad \ket{a^\prime}f(a^\prime)
		\label{chapter5.eqn11-scalar-product-continuous}
		\end{eqnarray}
		Where we have defined $f(a)$ as $f(a) = \braket{a}{f}$. In order for equation (\ref{chapter5.eqn11-scalar-product-continuous}) to be valid, we must have
		\begin{equation}
			\braket{a}{a^\prime} = \delta(a-a^\prime)
			\label{chapter5.eqn12-delta-func}
		\end{equation}
		for, with this choice, the right side of equation (\ref{chapter5.eqn11-scalar-product-continuous}) becomes equal to the left side:
		\begin{eqnarray}
			f(a) = \int da^\prime \quad \delta(a-a^\prime) f(a^\prime)
		\end{eqnarray}
		Thus, setting $a^\prime = a$ in equation (\ref{chapter5.eqn12-delta-func}) we find
		\begin{equation}
			\braket{a}{a} = \delta(0) = \infty
		\end{equation}
		In other words, the eigenkets $\{\ket{a}\}$ are not normalizable to unity since $\braket{a}{a}$ is not finite. Therefore, the eigenkets $\braket{a}{a}$ do not belong to the Hilbert space. However, we can include such eigenkets in the vector space, and the augmented vector space is called the \underline{physical Hilbert space}.
		
		
		
		The kets $\{\ket{a}\}$  are not physically realizable in the sense that no physical state of a system can have a state vector $\ket{\psi}$ which is one of the eigenkets $\ket{a}$. However, the set of eigenkets $\{\ket{a}\}$ can form a basis set because arbitrary ket $\ket{\psi}$ of finite norm can always be expanded in terms of $\{\ket{a}\}$.
		
		
		
		As a matter of terminology, we say that the eigenkets belonging to continuously varying eigenvalues of a hermitian operator are "normalizable" to a delta function, i.e. $\braket{a}{a^\prime} = \delta(a-a^\prime)$, even though the kets $\ket{a}$ are not normalizable in the strict mathematical sense, since
		\begin{equation}
			|| \ket{a} || = \infty
		\end{equation}
		In summary, for continuously varying eigenvalues, the orthogonality (\ref{chapter5.eqn13-orthogonality}) and completeness (\ref{chapter5.eqn13-completenss}) of the eigenvectors of a hermitian operator are written as
		\begin{eqnarray}
			\braket{a}{a^\prime} &= \delta(a-a^\prime)  \label{chapter5.eqn13-orthogonality}\\
			\hat{\mathbb{1}} &= \int da  \ket{a}\bra{a}  \label{chapter5.eqn13-completenss}
		\end{eqnarray}
		
		
	\section{Hermitian operator with continuous and discrete eigenvalue}
	The eigenvalue spectrum of a hermitian operator can be both discrete and continuous, In such a situation we have
	\begin{equation}
		\hat{A} \ket{a_i} = a_i \ket{a_i} ; \quad i=1,2,\ldots
	\end{equation}
	for discrete eigenvalues, and
	\begin{equation}
		\hat{A}\ket{a} = a\ket{a};\quad a\in D \subset R
	\end{equation}
	for continuous eigenvalues. The completeness condition is
	\begin{equation}
		\sum_i \ket{a_i}\bra{a_i} + \int da \ket{a\bra{a}} = \hat{\mathbb{1}}
	\end{equation}
	and the orthogonality condition are
	\begin{eqnarray}
		\braket{a_i}{a_j} = \delta_{ij} \\
		\braket{a}{a^\prime} =\delta(a-a^\prime)\\
		\braket{a_i}{a} = 0
	\end{eqnarray}
	
	\section{Problems}
	\begin{enumerate}
		\item Find the eigenvalues and the corresponding eigenvectors of the matrix
		\begin{equation}
			M = \left[\begin{matrix}
			1 & 1 \\ 0 & 1
			\end{matrix}\right]
		\end{equation}
		can this matrix be diagonalized?
		
		
		\textbf{Ans.}\newline
		The eigenvalue eqation is
		\begin{equation}
		\left[
			\begin{matrix}
				1 & 1 \\ 0 & 1
			\end{matrix}
			\right]
			\left[\begin{matrix}
			x_1 \\ x_2
			\end{matrix}\right] 
			= 
			\lambda 
			\left[\begin{matrix}
			x_1 \\ x_2
			\end{matrix}\right]
		\end{equation}
		The secular equation is then 
		\begin{eqnarray}
			\det(M - \lambda \mathbb{1}) &= 0 \nonumber\\
			\left|\begin{matrix}
			1-\lambda & 1 \\ 0 & 1-\lambda
			\end{matrix}\right| &=0 \nonumber \\
			\left(1-\lambda\right)^2 &= 0 \nonumber
		\end{eqnarray}
		i.e., $\lambda = 1,1$ (2 fold degeneracy)
		
		\underline{Eigenvector}
		
		
		With $\lambda=1$ the eigenvalue equation is
		\begin{eqnarray}
			\left[\begin{matrix}
				1 & 1 \\ 0 & 1
			\end{matrix}\right]
			\left[\begin{matrix}
				x_1 & x_2
			\end{matrix}\right]
			&= 1 \left[\begin{matrix}
			x_1 & x_2
			\end{matrix}\right] \nonumber\\
			\left[\begin{matrix}
				x_1+x_2 \\ x_2
			\end{matrix}\right]
			&= \left[\begin{matrix}
			x_1 & x_2
			\end{matrix}\right]
		\end{eqnarray}
		Thus
		\begin{eqnarray}
			x_1 + x_2 &= x_1 \nonumber \\
			x_2 &= 0 \nonumber
		\end{eqnarray}
		The element $x_1$ is arbitrary. Hence
		\begin{equation}
			\ket{1} = 
			\left[\begin{matrix}
				x_1 \\ 0
			\end{matrix}\right]
		\end{equation}
		Normalizing
		\begin{equation}
		\ket{1} = 
		\left[\begin{matrix}
		1 \\ 0
		\end{matrix}\right]
		\end{equation}
		We have found just one linearly independent eigenvector with $\lambda=1$. Since $M$ is not hermitian, there is no guarantee that there would be two linearly independent eigenvectors for a two fold degenerate eigenvalue. Here, for the given matrix $M$, which is non-hermitian, we have only one linearly independent eigenvector corresponding to the two-fold degenerate eigenvalue $\lambda=1$. So we do not have a complete set of eigenvectors of $M$ to span the two dimensional vector space. Hence $M$ is not diagonalizable by a change of basis, i.e., by a similarity transformation.
		
		

	\item	Find the eigenvalues and the corresponding eigenvectors of the matrix
	\begin{equation}
		A = \left[
		\begin{matrix}
			3 & \iu \\-\iu & 3
		\end{matrix}
		\right]
	\end{equation}
	\textbf{Ans.}\\
	 First note that $A^\dagger=A$, i.e., the matrix is hermitian. Hence the eigenvalues would be real and the eigenvectors belonging to distinct eigenvalues would be orthogonal. 
	 
	 The eigenvalue equation is
	 \begin{eqnarray}
		 \left[\begin{matrix}
			 3 & \iu \\ -\iu & 3
		 \end{matrix}\right]
		 \left[\begin{matrix}
		 x_1 \\ x_2
		 \end{matrix}\right]
		 &= \lambda \left[\begin{matrix}
		 x_1 \\ x_2
		 \end{matrix}\right] \nonumber \\
		 \left[\begin{matrix}
		 3-\lambda & \iu \\ -\iu & 3-\lambda
		 \end{matrix}\right]
		 \left[\begin{matrix}
		 x_1 \\ x_2
		 \end{matrix}\right]
		 &= \lambda \left[\begin{matrix}
		 0 \\ 0
		 \end{matrix}\right] 
		 \label{chapter5.eqn1-example2}
	 \end{eqnarray}
	 The secular equation is
	 \begin{eqnarray}
		 \left|\begin{matrix}
		 3-\lambda & \iu \\ -\iu & 3-\lambda
		 \end{matrix}\right| &= 0 \nonumber \\
		 (3-\lambda)^2 - (\iu)(-\iu) &= 0 \nonumber \\
		 (\lambda-3)^2 - 1 &= 0 \nonumber \\
		 (\lambda - 3 - 1)(\lambda - 3 + 1) &= 0 \nonumber \\
		 \lambda = 2,4
	 \end{eqnarray}
	None of the roots are degenerate.\\
	\underline{Eigenvector for $\lambda=2$}
	\begin{eqnarray}
		\left[\begin{matrix}
		3-2 & \iu \\ -\iu & 3-2
		\end{matrix}\right]
		\left[\begin{matrix}
		x_1 \\ x_2
		\end{matrix}\right]
		&= \oslash \nonumber \\
		\left[\begin{matrix}
		x_1 + \iu x_2 \\ -\iu x_1 + x_2
		\end{matrix}\right]
		&= \oslash \nonumber
	\end{eqnarray}
	Thus
	\begin{eqnarray}
		x_1 + \iu x_2 &= 0 \label{chapter5.eqn2-example2} \\
		-\iu x_1 + x_2 &= 0 \label{chapter5.eqn3-example2}
	\end{eqnarray}
	We get the solution $x_1 = -\iu x_2$. Taking $x_2$ to be arbitrary
	\begin{equation}
		\ket{2} = \left[\begin{matrix}
			-\iu x_2 \\ x_2
		\end{matrix}\right]
	\end{equation}
	Normalizing
	\begin{eqnarray}
		\braket{2}{2} &= 1 \nonumber \\
		\left[\begin{matrix}
		\iu x_2^* & x_2^*
		\end{matrix}\right]
				\left[\begin{matrix}
		-\iu x_2 \\ x_2
		\end{matrix}\right] 
		&=1 \nonumber \\
		2 \left|x_2\right|^2 &= 1 \nonumber \\
		\left|x_2\right| = \frac{1}{\sqrt{2}}
	\end{eqnarray}
	Take $x_2 \dot{=} \frac{1}{\sqrt{2}}$.\\
	We could have taken
	\begin{equation}
		x_2 = -\frac{1}{\sqrt{2}} \\
	\end{equation}
	or
	\begin{equation}
		x_2 = e^{\iu \phi} \frac{1}{\sqrt{2}} \\
	\end{equation}

	In all cases $\left|x_2\right| = \frac{1}{\sqrt{2}}$
	
	Normalized eigenvector $\ket{2}$ is
	\begin{equation}
		\ket{2} = \frac{1}{\sqrt{2}} \left[\begin{matrix}
			-\iu \\ 1
		\end{matrix}\right]
	\end{equation}
	
	\paragraph{Eigenvector for $\lambda=4$}
	\begin{eqnarray}
	\left[\begin{matrix}
	3-4 & \iu \\ -\iu & 3-4
	\end{matrix}\right]
	\left[\begin{matrix}
	x_1 \\ x_2
	\end{matrix}\right]
	&= \oslash \nonumber \\
	\left[\begin{matrix}
	-x_1 + \iu x_2 \\ -\iu x_1 - x_2
	\end{matrix}\right]
	&= \oslash \nonumber
	\end{eqnarray}
	Thus
	\begin{eqnarray}
	-x_1 + \iu x_2 &= 0 \label{chapter5.eqn5-example2} \\
	-\iu x_1 - x_2 &= 0 \label{chapter5.eqn6-example2}
	\end{eqnarray}
	We get the solution $x_1 \dot{=} \iu x_2$. Taking $x_2$ to be arbitrary
	\begin{equation}
	\ket{4} = \left[\begin{matrix}
	\iu x_2 \\ x_2
	\end{matrix}\right]
	\end{equation}
	The value of $x_2$ has to be found from normalization
	\begin{eqnarray}
	\braket{4}{4} &= 1 \nonumber \\
	\left[\begin{matrix}
	-\iu x_2^* & x_2^*
	\end{matrix}\right]
	\left[\begin{matrix}
	\iu x_2 \\ x_2
	\end{matrix}\right] 
	&=1 \nonumber \\
	2 \left|x_2\right|^2 &= 1 \nonumber \\
	\left|x_2\right| = \frac{1}{\sqrt{2}}
	\end{eqnarray}
	Take $x_2 = \frac{1}{\sqrt{2}}$.	
	Normalized eigenvector $\ket{4}$ is
	\begin{equation}
	\ket{4} = \frac{1}{\sqrt{2}} \left[\begin{matrix}
	\iu \\ 1
	\end{matrix}\right]
	\end{equation}
	\underline{Orthogonality of the eigenvectors}
	\begin{eqnarray}
	\ket{2} &= \frac{1}{\sqrt{2}} \left[\begin{matrix}
	-\iu \\ 1
	\end{matrix}\right] \\
	\ket{4} &= \frac{1}{\sqrt{2}} \left[\begin{matrix}
	\iu \\ 1
	\end{matrix}\right]
	\end{eqnarray}
	\begin{equation}
		\braket{2}{4} = \frac{1}{2} \left[\begin{matrix}
		\iu & 1
		\end{matrix}\right] \left[\begin{matrix}
		\iu \\ 1
		\end{matrix}\right] = \frac{1}{2} \left(\iu^2 + 1\right) = 0
	\end{equation}
	If we take $\ket{2}$ and $\ket{4}$ as the basis the matrix representation of $\hat{A}$ is
	\begin{equation}
		\hat{A} \rightarrow 
		\left[\begin{matrix}
			\bra{2}\hat{A}\ket{2} & \bra{2}\hat{A}\ket{4} \\
			\bra{4}\hat{A}\ket{2} & \bra{4}\hat{A}\ket{4}
		\end{matrix}\right] 
		= \left[\begin{matrix}
			2 & 0 \\ 0 & 4
		\end{matrix}\right]
	\end{equation}
	
	Now the similarity transformation that diagonalizes the matrix $A$. First the matrix $S$ that has the columns as the eigenvectors is
	\begin{equation}
		S = 
		\begin{blockarray}{ccc}
		\ & \ket{2} & \ket{4}\\
		\begin{block}{c[cc]}
		\ & -\iu/\sqrt{2} & \iu/\sqrt{2}  \\
		\ & 1/\sqrt{2} & 1/\sqrt{2}	\\
		\end{block}
		\end{blockarray}
	\end{equation}
	Therefore,
	\begin{eqnarray}
	A^\prime &= 
	\begin{blockarray}{ccc}
		\ & \ & \ \\
	\begin{block}{c[cc]}
	\bra{2} & \iu/\sqrt{2} & 1/\sqrt{2}  \\
	\bra{4} & -\iu/\sqrt{2} & 1/\sqrt{2}\\
	\end{block}
	\end{blockarray}
	A
	\begin{blockarray}{ccc}
	\ & \ket{2} & \ket{4}\\
	\begin{block}{c[cc]}
	\ & -\iu/\sqrt{2} & \iu/\sqrt{2}  \\
	\ & 1/\sqrt{2} & 1/\sqrt{2}	\\
	\end{block}
	\end{blockarray} \\
	&= 
	\begin{blockarray}{ccc}
	\ & \ & \ \\
	\begin{block}{c[cc]}
	\bra{2} & \iu/\sqrt{2} & 1/\sqrt{2}  \\
	\bra{4} & -\iu/\sqrt{2} & 1/\sqrt{2}\\
	\end{block}
	\end{blockarray}
	\left[
	\begin{matrix}
	3 & \iu \\ -\iu & 3
	\end{matrix}
	\right]
	\begin{blockarray}{ccc}
	\ & \ket{2} & \ket{4}\\
	\begin{block}{c[cc]}
	\ & -\iu/\sqrt{2} & \iu/\sqrt{2}  \\
	\ & 1/\sqrt{2} & 1/\sqrt{2}	\\
	\end{block}
	\end{blockarray}\\
	&= S^{-1} A S
	\end{eqnarray}
	
	\item	Find the eigenvalues and the corresponding eigenvectors of the matrix
	\begin{equation}
	M = \frac{1}{2}\left[
	\begin{matrix}
	3 & -1 & 0 \\
	-1 & 3 0 \\
	0 & 0 & 2
	\end{matrix}
	\right]
	\end{equation}
	\textbf{Ans.}\\
	The matrix $M$ is hermitian. Therefore the eigenvalues are real. The eigenvalues are obtained by solving the secular equation
	\begin{align*}
		\left|\begin{matrix}
		\frac{3}{2} - \lambda & -\frac{1}{2} & 0 \\
		-\frac{1}{2} & \frac{3}{2} -\lambda & 0 \\
		0 & 0 & 1-\lambda
		\end{matrix}\right| &= 0 \\
		(1-\lambda)\left[(\frac{3}{2}-\lambda)^2 - (-\frac{1}{2})(-\frac{1}{2})\right] &= 0 \\
		(\lambda-1)\left[(\lambda-3/2+1/2)(\lambda-3/2-1/2)\right] &= 0\\
		(\lambda-1)(\lambda-1)(\lambda-2) &= 0
	\end{align*}
	Thus the eigenvalues are $\lambda = 1,1,2$. The eigenvalue $1$ is two fold degenerate and the eigenvalue $2$ is non-degenerate, the two distinct eigenvalues are $\lambda_1=1$ with $g_1=2$ and $\lambda_2=2$ with $g_2=1$.
	
	\underline{Eigenvector for $\lambda=1$}\\
	Since $M$ is hermitian, there will be two linearly independent eigenvectors corresponding to $\lambda=1$. We will make the two linearly independent eigenvectors orthogonal. The eigenvalue equation is
	\begin{align*}
		\mathbb{A} \left[\begin{matrix}
		x_1 \\ x_2 \\ x_3
		\end{matrix}\right]
		&= 1 \left[\begin{matrix}
		x_1 \\ x_2 \\ x_3
		\end{matrix}\right] \\
		\left[\begin{matrix}
		\frac{3}{2}-1 & -\frac{1}{2} & 0 \\
		-\frac{1}{2} &\frac{3}{2}-1 & 0 \\
		0 & 0 & 1-1
		\end{matrix}\right]
		\left[\begin{matrix}
		x_1 \\ x_2 \\ x_3
		\end{matrix}\right] \\
		&=
		\left[\begin{matrix}
		0 \\ 0 \\ 0
		\end{matrix}\right] \\
		\left[\begin{matrix}
		\frac{1}{2} & -\frac{1}{2} & 0 \\
		-\frac{1}{2} &\frac{1}{2} & 0 \\
		0 & 0 & 0
		\end{matrix}\right]
		\left[\begin{matrix}
		x_1 \\ x_2 \\ x_3
		\end{matrix}\right] \\
		&=
		\left[\begin{matrix}
		0 \\ 0 \\ 0
		\end{matrix}\right]\\
		\left[\begin{matrix}
		1 & -1 & 0 \\
		-1 & 1 & 0 \\
		0 & 0 & 0
		\end{matrix}\right]
		\left[\begin{matrix}
		x_1 \\ x_2 \\ x_3
		\end{matrix}\right] \\
		&=
		\left[\begin{matrix}
		0 \\ 0 \\ 0
		\end{matrix}\right]
	\end{align*}
	Therefore $x_1=x_2=x$ (say) with arbitrary $x$. Also $x_3$ is arbitrary. Hence
	\begin{equation}
		\ket{1} = \left[\begin{matrix}
			x \\ x \\ x_3
		\end{matrix}\right]
	\end{equation}
	Choose $x=1$ and $x_3=0$ so that 
	\begin{equation}
		\ket{1}^{(1)} = \left[\begin{matrix}
		1 \\ 1 \\ 0
		\end{matrix}\right]
	\end{equation}
	Normalizing
	\begin{equation}
	\ket{1}^{(1)} = \frac{1}{\sqrt{2}}\left[\begin{matrix}
	1 \\ 1 \\ 0
	\end{matrix}\right] = \ket{\lambda=1,s=1}
	\end{equation}
	Next	Choose $x=0$ and $x_3=1$
	\begin{equation}
	\ket{1}^{(2)} = \left[\begin{matrix}
	0 \\ 0 \\ 1
	\end{matrix}\right] = \ket{\lambda=1,s=2}
	\end{equation}
	These are two orthogonal eigenvectors with eigenvalue $\lambda=1$.
	
	
	
	\underline{Eigenvector for $\lambda=2$}\\
	Here $g_{\lambda=2}=1$. The eigenvalue equation is
	\begin{align*}
		\left[\begin{matrix}
			\frac{3}{2}-2 & -\frac{1}{2} & 0 \\
			-\frac{1}{2} &\frac{3}{2}-2 & 0 \\
			0 & 0 & 1-2
		\end{matrix}\right]
		\left[\begin{matrix}
			x_1 \\ x_2 \\ x_3
		\end{matrix}\right] \\
		&=
		\left[\begin{matrix}
			0 \\ 0 \\ 0
		\end{matrix}\right] \\
		\left[\begin{matrix}
			-\frac{1}{2} & -\frac{1}{2} & 0 \\
			-\frac{1}{2} & -\frac{1}{2} & 0 \\
			0 & 0 & -1
		\end{matrix}\right]
		\left[\begin{matrix}
			x_1 \\ x_2 \\ x_3
		\end{matrix}\right]
	\end{align*}
	Therefore $x_1=-x_2=x$ (say) with arbitrary $x$. Also $x_3=0$. Therefore, eigenvector $\ket{2}$ is of the form
	\begin{equation}
		\ket{2} = \left[\begin{matrix}
			x \\ -x \\ 0
		\end{matrix}\right]
	\end{equation}
	
	Normalizing
	\begin{equation}
		\ket{2} = \frac{1}{\sqrt{2}}\left[\begin{matrix}
			1 \\ -1 \\ 0
		\end{matrix}\right] = \ket{\lambda=1,s=1}
	\end{equation}
	
	\textbf{Similarity Transformation}
	\begin{equation}
		M^\prime = S M S^\dagger
	\end{equation}
	Now
	\begin{equation}
		S^\dagger =
		 \begin{blockarray}{cccc}
		\ & \ket{1,1} & \ket{1,2} & \ket{2}\\
		\begin{block}{c[ccc]}
		\ & \frac{1}{\sqrt{2}} & 0 & \frac{1}{\sqrt{2}}  \\
		\ & \frac{1}{\sqrt{2}} & 0 & -\frac{1}{\sqrt{2}}  \\
		\ & 0 & 1 & 0  \\
		\end{block}
		\end{blockarray} \\
	\end{equation}
	\begin{equation}
		S = \left(S^\dagger\right)^\dagger = 
 		\begin{blockarray}{cccc}
		\begin{block}{c[ccc]}
		\bra{1,1} & \frac{1}{\sqrt{2}} & \frac{1}{\sqrt{2}} & 0  \\
		\bra{1,2} & 0 & 0 & 1  \\
		\bra{2} & \frac{1}{\sqrt{2}} & -\frac{1}{\sqrt{2}} & 0  \\
		\end{block}
		\end{blockarray} \\
	\end{equation}
	Then the matrix $M^\prime$ is diagonal
	\begin{equation}
		\mathbb{M}^\prime = \mathbb{S}\mathbb{M}\mathbb{S}^\dagger = \left[\begin{matrix}
		1 & 0 & 0 \\ 0 & 1 & 0 \\ 0 & 0 & 2
		\end{matrix}\right]
	\end{equation}

	


	\end{enumerate}
