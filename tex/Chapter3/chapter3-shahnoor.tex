%\chapter{sheet-3 : Linear Vector Space (continued)}



\section{Operators in a Hilbert Space}
An operator is a prescription by which every vector $\psi_a$ in a Hilbert space $H$ is associated with another vector $\psi_b$ in the space:
\begin{equation}\label{eqn3.1}
\hat{A} : \psi_1 \rightarrow \psi_b
\end{equation}
for $\psi_a , \psi_b \in H$. We usually employ the notation
\begin{equation}\label{eqn3.2}
\psi_b = \hat{A}\psi_a
\end{equation}
In Dirac notation, we write
\begin{equation}\label{eqn3.3}
\ket{b} = \hat{A}\ket{a}
\end{equation}
where both $\ket{a}$ and $\ket{b}$ belong to the ket-space. An operator can also act on a bra vector (bra-space is also a Hilbert space; it is dual to the ket space) changing it to another bra-vector. The notation we employ is
\begin{equation}\label{eqn3.4}
\bra{\psi} = \bra{\phi} \hat{A}
\end{equation}
Here the operator $\hat{A}$ acts on the bra-vector $\bra{\phi}$ to produce the bra vector $\bra{\psi}$. We place the bra-vector on which the operator acts on the left of the operator.

\section{Linear operators}
An operator $\hat{A}$ is said to be a linear operator if it has the following property : 
\begin{enumerate}
	\item 
	For any vector $\ket{a}$ and $\ket{b}$ and any complex number $\lambda_1$ and $\lambda_2$, we have
	\begin{equation}\label{eqn3.5}
	\hat{A} (\lambda_1 \ket{a} + \lambda_2 \ket{b}) = \lambda_1 \hat{A} \ket{a} + \lambda_2 \hat{A} \ket{b}
	\end{equation}
	A linear vector operator can act on a bra vector also
	\begin{equation}\label{eqn3.6}
	(\lambda_1 \bra{a} + \lambda_2 \bra{b}) \hat{A} = \lambda_1 \bra{a} \hat{A} + \lambda_2 \bra{b} \hat{A}
	\end{equation}
	
	\item 
	The operator $\hat{A}$ is anti linear if
	\begin{equation}\label{eqn3.7}
	\hat{A} (\lambda_1 \ket{a} + \lambda_2 \ket{b}) = \lambda_1^* \hat{A} \ket{a} + \lambda_2^* \hat{A} \ket{b}
	\end{equation}
	
	\item 
	Two operators $\hat{A}$ and $\hat{B}$ are equal if 
	\begin{equation}\label{eqn3.8}
	\hat{A} \ket{\psi} = \hat{B} \ket{\psi}
	\end{equation}
	for all $\ket{\psi}$ in the vector space.
	
	\item 
	Sum of two operators $\hat{A}$ and $\hat{B}$ is defined as
	\begin{equation}\label{eqn3.9}
	(\hat{A} + \hat{B}) \ket{\psi} = \hat{A} \ket{\psi} + \hat{B} \ket{\psi}
	\end{equation}
	
	\item 
	Product of two operators $\hat{A}$ and $\hat{B}$ is defined as
	\begin{equation}\label{eqn3.10}
	(\hat{A} \hat{B}) \ket{\psi} = 	\hat{A} (\hat{B} \ket{\psi})
	\end{equation}
	This equation says that the operator $\hat{A} \hat{B}$ acting on $\ket{\psi}$ produces the same vector which would be obtained if we first let $\hat{B}$ act on $\ket{\psi}$ and then $\hat{A}$ acts on the resultant of the previous operation. In general $\hat{A}\hat{B} \neq \hat{B}\hat{A}$, although in exceptional cases we may have $\hat{A}\hat{B} = \hat{B}\hat{A}$.
	
\end{enumerate}

\section{Commutator of two operators}
The commutator of two operators $\hat{A}$ and $\hat{B}$ is defined as
\begin{equation}\label{eqn3.11}
[\hat{A}, \hat{B}] _\equiv^{def} \hat{A}\hat{B} - \hat{B}\hat{A}
\end{equation}
In general $[\hat{A}, \hat{B}] \neq 0 $ (null operator). If $[\hat{A}, \hat{B}] = 0$, we say that $\hat{A}$ and $\hat{B}$ commute with each other.

\subsection{Some properties of commutators}
\begin{align}\label{eqn3.12-3.17}
	[\hat{A}, \lambda \hat{B}] &= \lambda [\hat{A}, \hat{B}] \\
	[\lambda \hat{A}, \hat{B}] &= \lambda [\hat{A}, \hat{B}] \\
	[\hat{A}, \hat{B} + \hat{C}] &= [\hat{A}, \hat{B}] + [\hat{A}, \hat{C}]\\
	[\hat{A}, \hat{B}] &= - [\hat{B}, \hat{A}]\\
	[\hat{A}, \hat{B}\hat{C}] &= \hat{B}[\hat{A}, \hat{C}] + [\hat{A}, \hat{B}] \hat{C}\\
	[\hat{A}\hat{B}, \hat{C}] &= \hat{A}[\hat{B}, \hat{C}] + [\hat{A}, \hat{C}] \hat{B}
\end{align}
\section{Projection operator}
(An important example of a linear operator).\\
Consider the operator $\hat{P_a}$ defined as
\begin{equation}\label{eqn3.18}
\hat{P_a} = \ket{a} \bra{a}
\end{equation}
where
\begin{equation}\label{eqn3.19}
\braket{a}{b} = 1
\end{equation}
Operating by $\hat{P_a}$ on an arbitrary ket $\ket{\psi}$, we have
\begin{equation}\label{eqn3.20}
\hat{P_a}\ket{\psi} = \ket{a}\bra{a}\ket{\psi}
\end{equation}
i.e., $\hat{P_a}$ projects the ket $\ket{\psi}$ along $\ket{a}$. The complex number $\braket{a}{\psi}$ is the component of $\ket{\psi}$ along $\ket{a}$.\\
Now, $\ket{P_a}$ is a linear operator. To show this consider
\begin{align}\label{eqn3.21-3.23}
	\hat{P_a} \left(\lambda_1\ket{\psi_1} + \lambda_2\ket{\psi_2
	}\right) &= \ket{a}\bra{a} \left(\lambda_1\ket{\psi_1} + \lambda_2\ket{\psi_2}\right)\\
	&= \lambda_1\ket{a}\bra{a} \ket{\psi_1} + \lambda_2\ket{a}\bra{a} \ket{\psi_2}\\
	&= \lambda_1\hat{P_a} \ket{\psi_1} + \lambda_2\hat{P_a} \ket{\psi_2}
\end{align}
Another important property of the projection operator is
\begin{equation}\label{eqn3.24}
\hat{P_a}^2 = \hat{P_a}
\end{equation}
To prove this allow $\hat{P_a}^2$ to act on a ket.
\begin{align}\label{eqn3.25-3.29}
	\hat{P_a}^2 \ket{\psi} &= \hat{P_a} \hat{P_a} \ket{\psi} \\
	&= \hat{P_a} \ket{a}\braket{a}{\psi} \\
	&= \ket{a} \braket{a}{a}\braket{a}{\psi} \\
	&= \ket{a} \braket{a}{\psi} \\
	&= \hat{P_a}\ket{\psi}
\end{align}
\underline{Exercise} Six operator are defined as follows:
\begin{align}\label{eqn3.30-3.35}
	\hat{A_1} \psi(x) &= [\psi(x)]^2 \\
	\hat{A_2} \psi(x) &= \frac{d}{dx}\psi(x) \\
	\hat{A_3} \psi(x) &= \int_{a}^{x}\psi(x^\prime) dx^\prime\\
	\hat{A_4} \psi(x) &=  x^2 \psi(x) \\
	\hat{A_5} \psi(x) &= \sin(\psi(x)) \\
	\hat{A_6} \psi(x) &= \frac{d^2}{dx^2}\psi(x)
\end{align}
which of these operators $\hat{A_i}$ are linear operator.



\section{Representation of vectors and operators}
Let ${\phi_i}$ be a complete orthonormal basis set in a Hilbert space. Since the basis is orthonormal, we must have
\begin{equation}\label{eqn3.36}
(\phi_i,\phi_j) = \delta_{ij}
\end{equation}
An arbitrary vector $\psi_a$ can be written as a linear combination of the basis vectors.\\
We write
\begin{equation}\label{eqn3.37}
\psi_a = \sum_{i} a_i\phi_i
\end{equation}
Where the scalars $a_i$ are the components of the vector $\psi_a$ along the basis vectors $\phi_i$. Using the orthonormality of the basis vectors we immediately have
\begin{equation}\label{eqn3.38}
a_i = (\phi_i, \psi_a)
\end{equation}
We can arrange these numbers as a column matrix
\begin{equation}\label{eqn3.39}
\left(
\begin{matrix}
a_1 \\ a_2 \\ \vdots
\end{matrix}
\right) \equiv \left(
\begin{matrix}
(\phi_1, \psi_a) \\
(\phi_2, \psi_a) \\
\vdots
\end{matrix}
\right)
\end{equation}
This column matrix is called the representation of the vector $\psi_a$ with respect to the given basis $\{\phi_i \}$.\\
In Dirac notation we represent the vector $\psi_a$ as $\ket{a}$ and the basis vectors $\phi_i$ are written as $\ket{i}$. We can expand a general ket $\ket{a}$ as a linear combination of the basis kets:
\begin{equation}\label{eqn3.40}
\ket{a} = \sum_{i=1}^{\infty} a_i \ket{i}
\end{equation}
Orthonormality of the basis kets can be written as
\begin{equation}\label{eqn3.41}
\braket{i}{j} = \delta_{ij}
\end{equation}
The complex scalar $a_i$ are called the components of the ket $\ket{a}$ along $\ket{i}$. Using the orthonormality condition of the basis vectors (\ref{eqn3.40}), we have 
\begin{equation}\label{eqn3.42}
a_i = \braket{i}{\psi}
\end{equation}
These scalars $a_1, a_2, \ldots$ arranged as a column matrix is called the representation of $\ket{a}$ in the basis $\{\ket{i} \}, i=1,2,\ldots$.\\
Thus
\begin{equation}\label{eqn3.43}
\ket{a} \rightarrow \left(
\begin{matrix}
a_1 \\ a_2 \\ \vdots
\end{matrix}
\right) \equiv \left(
\begin{matrix}
\braket{1}{a} \\ \braket{2}{a} \\ \vdots
\end{matrix}
\right)
\end{equation}
We can write down the representation of any one of the basis vectors in the same basis as
\begin{eqnarray}\label{eqn3.44}
\ket{1} \rightarrow \left(
\begin{matrix}
1 \\ 0 \\ 0 \\ \vdots
\end{matrix}
\right)
&
\ket{2} \rightarrow \left(
\begin{matrix}
0 \\ 1 \\ 0 \\ \vdots
\end{matrix}
\right)
&
\ket{3} \rightarrow \left(
\begin{matrix}
0 \\ 0 \\ 1 \\ \vdots
\end{matrix}
\right)
\end{eqnarray}
and so on.\\
Now, using equation \ref{eqn:3.42} in equation \ref{eqn:3.40} we have
\begin{align}\label{eqn3.45-3.48}
	\ket{a} &= \sum_{i} a_i \ket{i} \\
	&= \sum_{i} \braket{i}{a} \ket{i} \\
	&= \sum_{i} \ket{i}\braket{i}{a}\\
	&= \left(\sum_{i} \hat{P_i}\right)\ket{a}
\end{align}
where
\begin{equation}\label{eqn3.49}
\hat{P_i} = \ket{i}\bra{i}
\end{equation}
is the projection operator along $\ket{i}$. Since equation \ref{eqn3.45-3.48} is true for all $\ket{a}$ in the vector space (this is because $\{\ket{i}\}$ form a complete set), we must have
\begin{equation}\label{eqn3.50}
\sum_{i} \hat{P_i} = \sum_{i}\ket{i} \bra{i} = \hat{\mathbb{I}}
\end{equation}
where $\mathbb{I}$ is the identity operator. Equation \ref{eqn3.50} is called the \textbf{completeness condition} for the basis vectors.


\section{Matrix representation of ket and bra vectors}
The ket vector $\ket{a}$ is representation by a column matrix
\begin{equation}\label{eqn3.51}
\left(
\begin{matrix}
a_1 \\ a_2 \\ \vdots
\end{matrix}
\right) \equiv \left(
\begin{matrix}
\braket{1}{a} \\ \braket{2}{a} \\ \vdot
\end{matrix}
\right)
\end{equation}
in a basis $\{\ket{i} \}$. The dual of ket $\ket{a}$ is the $\bra{a}$. But what is the matrix  representation of the bra $\bra{a}$ in the same basis? To see this we can expand $\bra{a}$ as
\begin{equation}\label{eqn3.52}
\bra{a} = \sum_{i} \braket{a}{i}\bra{i}
\end{equation}
The $\bra{a}$ is represented by a row vector:
\begin{equation}\label{eqn3.53}
\bra{a} \rightarrow \left(
\begin{matrix}
\braket{a}{1} & \braket{a}{2} & \ldots
\end{matrix}
\right) = \left(
\begin{matrix}
a_1^* & a_2^* & \ldots
\end{matrix}
\right)
\end{equation}
Then the scalar product becomes a number. Thus
\begin{align}\label{eqn3.54-3.57}
	\braket{a}{a} &= \left(
	\begin{matrix}
		a_1^* & a_2^* & \ldots
	\end{matrix}
	\right) \left(
	\begin{matrix}
		a_1 \\ a_2 \\ \vdots
	\end{matrix}
	\right)\\
	&= a_1^* a_1 + a_2^* a_2 + \ldots \\
	&= \sum_{i} a_i^* a_i \\
	&= \sum_{i} |a_i|^2 
\end{align}
Here the quantity $|a_i|^2$ is just a number.\\
In general
\begin{align}\label{eqn3.57-3.62}
	\braket{b}{a} &= \sum_{i} \braket{b}{i}\braket{i}{a}			\\
	&= \left(
	\begin{matrix}
		\braket{b}{1} & \braket{b}{2} & \ldots
	\end{matrix}
	\right) \left(
	\begin{matrix}
		\braket{1}{a} \\ \braket{2}{a} \\ \vdots
	\end{matrix}
	\right)\\
	&= \left(
	\begin{matrix}
		b_1^* & b_2^* & \ldots
	\end{matrix}
	\right) \left(
	\begin{matrix}
		a_1 \\ a_2 \\ \vdots
	\end{matrix}
	\right)\\
	&= b_1^* a_1 + b_2^* a_2 + \ldots \\
	&= \sum_{i} b_i^* a_i
\end{align}
Here the quantity $b_i^* a_i$ is complex number.


\section{Representation of an operator in a basis}
Consider the equation
\begin{equation}\label{eqn3.63}
\ket{b} = \hat{A} \ket{a}
\end{equation}
Let $\{\ket{i} \}$ where $i=1,2,\ldots$ be a complete set of orthonormal basis states. Taking the component of equation \ref{eqn3.63} along $\ket{i}$,  we have
\begin{align}\label{eqn3.64-3.65}
	\braket{i}{b} &= \bra{i}\hat{A}\ket{a}\\
	&= \sum_{j} \bra{i}\hat{A}\ket{j}\braket{j}{a}
\end{align}
In matrix notation
\begin{equation}\label{eqn3.66}
b_i = \sum_{j} A_{i j} a_j
\end{equation}
where 
\begin{align}\label{eqn3.67-3.69}
	b_i &\equiv \braket{i}{b} \\
	a_j &\equiv \braket{j}{a} \\
	A_{i j} &\equiv	\bra{i}\hat{A}\ket{j}
\end{align}
Writing in full, equation \ref{eqn3.66} becomes
\begin{equation}\label{eqn3.70}
\left(
\begin{matrix}
b_1 \\ b_2 \\ . \\ . \\ .
\end{matrix}
\right) = \left(
\begin{matrix}
A_{11} & A_{12} & \ldots \\
A_{21} & A_{22} & \ldots \\
. & . & \ldots \\
. & . & \ldots \\
. & . & \ldots \\
\end{matrix}
\right) \left(
\begin{matrix}
a_1 \\ a_2 \\ . \\ . \\ .
\end{matrix}
\right)
\end{equation}
The matrix $[A]$ with elements $A_{i j} = \bra{i}\hat{A}\ket{j}$ is called the matrix representation of the operator $\hat{A}$ with respect to the given basis $\{\ket{i} \}$ . Using a basis set, the operator $\hat{A}$ can also be written as 
\begin{align}\label{eqn3.71-3.74}
	\hat{A} &= \hat{\mathbb{I}} \hat{A} \hat{\mathbb{I}} \\
	&= \left(\sum_{i}\ket{i}\bra{i} \right) \hat{A} \left(\sum_{j}\ket{j}\bra{j} \right)\\
	&= \sum_{i, j}\ket{i}\bra{i} \hat{A}\ket{j}\bra{j}\\
	&= \sum_{i, j}\ket{i} A_{i j} \bra{j}
\end{align}

\section{Matrix representation of the sum and product of two operators}
Let 
\begin{equation}\label{eqn3.75}
\hat{C} = \hat{A} + \hat{B}
\end{equation}
Then
\begin{align}\label{eqn3.76-3.79}
	C_{ij} &= \bra{i} \hat{C} \ket{j}\\
	&= \bra{i} \hat{A} + \hat{B} \ket{j}\\
	&= \bra{i} \hat{A} \ket{j} + \bra{i} \hat{B} \ket{j}\\
	&= A_{ij} + B_{ij}
\end{align}
Next, let
$\therefore$
\begin{align}\label{eqn3.80-3.84}
	C_{ij} &= \bra{i} \hat{C} \ket{j}\\
	&= \bra{i} \hat{A}\hat{B} \ket{j}\\
	&= \bra{i} \hat{A}\hat{\mathbb{I}} \hat{B} \ket{j}\\
	&= \sum_{k} \bra{i} \hat{A}\ket{k} \bra{k} \hat{B} \ket{j}\\
	&= \sum_{k} A_{ik} B_{kj}
\end{align}
In full matrix form
\begin{equation}\label{eqn3.85}
[C] = [A] [B]
\end{equation}
where
\begin{equation}\label{eqn3.86}
[A] = \left(
\begin{matrix}
\bra{1}\hat{A}\ket{1} & \bra{1}\hat{A}\ket{2} & \ldots \\
\bra{2}\hat{A}\ket{1} & \bra{2}\hat{A}\ket{2} & \ldots \\
\vdots & \vdots & \vdots
\end{matrix}
\right)
\end{equation}
and similar expression applies for $[B]$ and $[C]$.
This result shows that the matrix of an operator product is equal to the product of the matrices representing the operators, taken in the same order.


\begin{enumerate}[label=\textbf{Example \arabic*},start=1]
	\item 
	Using a basis set $\{\ket{i} \}$ write down $\bra{b}\hat{A}\ket{a}$ as a matrix product.\\
	\underline{Ans}
	\begin{align}\label{eqn3.87-3.89}
		\bra{b}\hat{A}\ket{a} 
		&= \sum_{i, j} \braket{b}{i} \bra{i}\hat{A}\ket{j}\braket{j}{a}\\
		&=\sum_{i, j} b_i^* A_{i j} a_j \\
		&= [b]^\dagger [A] [a]
	\end{align}
	where $[b]^\dagger$ is the matrix representation of $\bra{b}$.
	\begin{equation}\label{eqn3.90}
	[b]^\dagger = \left(
	\begin{matrix}
	b_1^* & b_2^* & \ldots
	\end{matrix}
	\right)
	\end{equation}
	$[A]$ is the matrix representation of the operator $\hat{A}$
	\begin{equation}\label{eqn3.91}
	[A] = \left[
	\begin{matrix}
	A_{11} & A_{12} & \ldots \\
	A_{21} & A_{22} & \ldots \\
	\vdots & \vdots & \vdots
	\end{matrix}
	\right]
	\end{equation}
	and $[a]$ is the matrix representation of the ket $\ket{a}$.
	\begin{equation}\label{eqn3.92}
	[a] = \left(
	\begin{matrix}
	a_1 \\ a_2 \\ \vdots
	\end{matrix}
	\right)
	\end{equation}
	writing in full
	\begin{equation}\label{eqn3.93}
	\bra{b} \hat{A} \ket{a} = \left(
	\begin{matrix}
	b_1^* & b_2^* & \ldots
	\end{matrix}
	\right) \left(
	\begin{matrix}
	A_{11} & A_{12} & \ldots \\
	A_{21} & A_{22} & \ldots \\
	\vdots & \vdots & \vdots
	\end{matrix}
	\right)\left(
	\begin{matrix}
	a_1 \\ a_2 \\ \vdots
	\end{matrix}
	\right)
	\end{equation}
\end{enumerate}






