
\chapter{sheet-2 : Linear Vector Space}
References
\begin{enumerate}
	\item 
	Quantum Mechanics	-	Shanker
	
	\item
	Quantum Mechanics	- Sakurai
\end{enumerate}

\section{Definition}
A linear vector space $V$ is a collection of objects $\psi_a, \psi_b, \ldots$, called vectors, which satisfy the following postulates:
\begin{enumerate}
	\item 
	If $\psi_a$ and $\psi_b$ are vectors in $V$, there is a unique vector $\psi_a + \psi_b$ in $V$, called the sum of $\psi_a $ and $\psi_b$. In other words, and operation called addition is defined in the vector space such that the space is closed under addition.
	
	\item 
	The vector addition is commutative and associative, i.e.,
	\begin{eqnarray}\label{eqn:2.1-2.2}
		\psi_a + \psi_b &= \psi_b + \psi_a \\
		\psi_a + (\psi_b + \psi_c) &= (\psi_a + \psi_b) + \psi_c
	\end{eqnarray}
	
	\item 
	There is a vector in $V$ called the null vector and denoted by $\phi$ satisfying
	\begin{equation}\label{eqn:2.3}
	\psi_a + \phi = \phi + \psi_a
	\end{equation}
	for every $\psi_a$ in $V$.
	
	\item 
	For every vector $\psi_a$ in $V$ there is another vector $\psi_a^\prime$ in $V$ such that
	\begin{equation}\label{eqn:2.4}
	\psi_a + \psi_a^\prime = \phi
	\end{equation}
	we denote $\psi_a^\prime$ as $-\psi_a$.\\
	(Note : we use the notation $\psi_a - \psi_b$ to mean $\psi_a + (-\psi_b$).
	
	\item 
	If $\psi_a$ is a vector and $\lambda$ is an arbitrary number (real or complex), called a scaler, there is a uniquely defined vector $\lambda \psi_a$ in $V$ satisfying.
	\begin{enumerate}
		\item 
		\begin{equation}\label{eqn:2.5}
		\lambda (\psi_a  + \psi_b) = \lambda \psi_a + \lambda + \psi_b
		\end{equation}
		i.e., multiplication is distributive with respect to vector addition.
		
		\item 
		\begin{equation}\label{eqn:2.6}
		(\lambda \mu) \psi_a = \lambda (\mu \psi_a)
		\end{equation}
		i.e., multiplication  by a scalar is associative.
		
		\item 
		\begin{equation}\label{eqn:2.7}
		(\lambda + \mu) \psi_a = \lambda \psi_a + \mu \psi_b
		\end{equation}
		i.e., multiplication is distributive with respect to addition scalars.
		
		\item 
		Multiplication by scalars $0$ and $1$ are defined by
		\begin{eqnarray}\label{eqn:2.8-2.9}
			0 \psi_a = \phi \\
			1 \psi_a = \psi_a
		\end{eqnarray}
		for any $\psi_a$ in $V$.
	\end{enumerate}
\end{enumerate}
\section{Example of linear vector space}
\begin{enumerate}
	\item 
	Consider all real numbers $x$ in the range $-\infty$ to $\infty$, i.e.,
	\begin{eqnarray}\label{eqn:2.10-2.11}
		-\infty < x < \infty \\
		x \in \Re \ \ \text{ ($\Re$ is the set of all real numbers )}
	\end{eqnarray}
	Take any two real numbers $x_1$ and $x_2$. If we add two real numbers we get another real number in $\Re$. Thus
	\begin{equation}\label{eqn:2.12}
	x_1 + x_2 = \Re
	\end{equation}
	Next take any real number $x$. If we multiply $x$ by another real number $\lambda$, we get a real number in $\Re$, i.e.,
	\begin{equation}\label{eqn:2.13}
	\lambda x \in \Re
	\end{equation}
	If we take a real number $x$, then there exists another real number $-x$ such that
	\begin{equation}\label{eqn:2.14}
	x + (-x) = 0
	\end{equation}
	So the real numbers form a vector space with the real numbers themselves as vectors in the space. The number $0$ is the null vector $\phi$ of the  space. The scalars $\lambda$ by which the vectors are multiplied are also real numbers.\\
	Thus the real numbers form a real linear vector space over a field which are also real numbers. The addition and multiplication are just the normal addition and multiplication of the real numbers.
	
	\item 
	The set of $n-tuples$ of numbers $(x_1, x_2, \ldots, x_n)$.
	When the addition of vectors and multiplication by a scalar are defined by
	\begin{equation}\label{eqn:2.15}
	(x_1 , x_2, \ldots, x_n) + (y_1 , y_2, \ldots, y_n) = (x_1 + y_1, x_2 + y_2, \ldots, x_n + y_n)
	\end{equation}
	and 
	\begin{equation}\label{eqn:2.16}
	\lambda (x_1, x_2, \ldots, x_n) = (\lambda x_1, \lambda x_2, \ldots, \lambda x_n)
	\end{equation}
	
	
	\item 
	The collection of all square-integrable complex valued functions of a real variable form a vector space. consider all functions
	\begin{equation}\label{eqn:2.17}
	f:\Re \rightarrow \mathbb{C}
	\end{equation}
	here, 
	$\Re $ =  set of real numbers \\
	$\mathbb{C}$  = set of complex numbers\\
	such that
	\begin{equation}\label{eqn:2.18}
	\int_{-\infty}^{\infty} f^*(x) f(x) dx \equiv \int_{-\infty}^{\infty} |f(x)|^2 dx < \infty \text{\  \ (i.e., finite)}
	\end{equation}
	The sum of two functions and the product of a function by a complex scalar are defined in the usual way.\\
	The reason the square-integrable functions form a (complex) vector space is that the space is closed under addition. The vectors of the space are the square-integrable functions. In other words, it can be shown that if $f$ and $g$ are two vectors, in this case two functions $f(x)$ and $g(x)$ both of which are square integrable, then $f(x) + g(x)$ is also square integrable and have the sum belong to the vector space.\\
	\textbf{proof:} \\
	Let $f(x)$ and $g(x)$ be two square integrable function, i.e., 
	\begin{equation}\label{eqn:2.19}
	\int_{-\infty}^{\infty} |f(x)|^2 dx < \infty
	\end{equation}
	and
	\begin{equation}\label{eqn:2.20}
	\int_{-\infty}^{\infty} |g(x)|^2 dx < \infty
	\end{equation}
	then using the inequality
	\begin{equation}\label{eqn:2.21}
	\int_{-\infty}^{\infty} |f+g|^2 dx \leq \left[\sqrt{\int_{\infty}^{\infty} |f|^2 dx} \sqrt{\int_{\infty}^{\infty} |g|^2 dx}\right]^2
	\end{equation}
	it is obvious that
	\begin{equation}\label{eqn:2.22}
	\int_{-\infty}^{\infty} |f+g|^2 dx < \infty
	\end{equation}
	(i.e., finite).
	
	\item 
	The set of all $n\times n$ matrices with complex elements form a complex linear vector space. For illustation let us take $2\times 2$ matrix $A$
	\begin{equation}\label{eqn:2.23}
	A = \left(			
	\begin{matrix}
	a & b \\
	c & d
	\end{matrix}
	\right)
	\end{equation}
	where the elements $a, b, c$ and $d$ can, in general, be complex. Then $A$ belongs to a vector (complex) space.
	We have
	
	\begin{eqnarray}\label{eqn:2.24-2.27}
		\phi &= \left(
		\begin{matrix}
			0 & 0 \\ 0 & 0
		\end{matrix}
		\right)\\
		\mathbb{I} &= \left(
		\begin{matrix}
			1 & 0 \\ 0 & 1
		\end{matrix}
		\right)\\
		-A &= \left(
		\begin{matrix}
			-a & -b \\ -c & -d
		\end{matrix}
		\right)\\
		\lambda A &= \left(
		\begin{matrix}
			\lambda a & \lambda b \\ \lambda c & \lambda d
		\end{matrix}
		\right)
	\end{eqnarray}
	The set of all these matrices form a vector space.						
\end{enumerate}

\section{Inner product space or a unitary vector space}\label{inner_product}
For a general linear vector space, product of vectors (i.e. multiplication of two vectors) need ot be defined. However, we will restrict ourselves to spaces in which a  scalar product or an inner product is defined.\\
A linear vector space is called unitary if a scalar product is defined in it. To every pair of vectors $\psi_a$ and $\psi_b$ in $V$ there corresponds a unique scalar (in general complex), called the scalar product. The scalar product is defined to have the following properties :
\begin{eqnarray}\label{eqn:2.28-2.32}
	(\psi_a , \psi_b) &= (\psi_b, \psi_a)^* \\
	(\psi_a , \lambda \psi_b) &= \lambda (\psi_a , \psi_b) \\
	(\lambda \psi_a , \psi_b) &= \lambda^* (\psi_a , \psi_b) \\
	(\psi_a , \psi_b + \psi_c) &= (\psi_a , \psi_b) + (\psi_a , \psi_c) \\
	(psi_a, \psi_a) \geq 0 \ \text{;the equality holds only if $\psi_a$ is the null vector}
\end{eqnarray}
It follows from the above postulated properties of the scalar product, that the scalar product is linear with respect to post factors, i.e.,
\begin{equation}\label{eqn:2.33}
(\psi_a, \lambda \psi_b + \mu \psi_c) = \lambda (\psi_a , \psi_b) + \mu (\psi_a , \psi_c)
\end{equation}
and anti-linear with respect to the pre factors, i.e.,
\begin{equation}\label{eqn:2.34}
(\lambda \psi_a + \mu \psi_b, \psi_c ) = \lambda^* (\psi_a , \psi_c) + \mu^* (\psi_b , \psi_c)
\end{equation}

\section{Examples of scalar product}
\begin{enumerate}[label=\textbf{Example \arabic*},start=1]
	\item 
	Consider the vector space consisting of all square integrable functions of a real variable in the domain $[a,b]$. This space is denoted by $L^2[a,b]$.\\
	Suppose
	\begin{equation}\label{eqn:2.35}
	f \in L^2[a,b]
	\end{equation}
	i.e.,
	\begin{equation}\label{eqn:2.36}
	\int_{a}^{b} f^*(x)f(x) dx \equiv \int_{a}^{b} |f(x)|^2 dx < \infty
	\end{equation}
	We can define the scalar product of two vectors $f$ and $g$ as
	\begin{equation}\label{eqn:2.37}
	(f, g) _\equiv^{def} \int_{a}^{b} f^*(x) g(x) dx = complex \ number
	\end{equation}
	We can show
	\begin{equation}\label{eqn:2.38}
	\left|(f, g)\right| = \left[\sqrt{\int_{a}^{b} |f(x)|^2 dx}\right] \left[\sqrt{\int_{a}^{b} |g(x)|^2 dx}\right]
	\end{equation}
	Since both $f$ and $g$ are square integrable, $|(f, g)|$ is finite, i.e., the scalar product of $f$ and $g$ exists.\\
	The scalar product defined above satisfies all the properties that a scalar product is postulated to have.
	
	
	\item 
	Now consider the vector space consisting of $n-tuples$ of complex numbers. Such a vector space is denoted as $\mathbb{C}^n$.\\
	A vector $\psi_a \in \mathbb{C}^n$ may be expressed as
	\begin{equation}\label{eqn:2.39}
	\psi_a = \left(
	\begin{matrix}
	a_1 & a_2 & \ldots & a_n
	\end{matrix}
	\right)^T
	\end{equation}
	The scalar product may then be defined as
	\begin{eqnarray}\label{eqn:2.40-2.41}
		(\psi_a , \psi_b) &_\equiv^{def} \left(
		\begin{matrix}
			a_1^* & a_2^* & \ldots &  a_3^*
		\end{matrix}
		\right)\left(
		\begin{matrix}
			b_1 \\ b_1 \\ \vdots \\ b_n
		\end{matrix}
		\right)\\
		&= \sum_{i=1}^{n} a_i^* b_i
	\end{eqnarray}
	This scalar product also satisfies all the properties of a scalar product.
	
	
	\item 
	Euclidean 3-space $\mathbb{R}^3$. The vectors of $\mathbb{R}^3$ are $3-tuples$ of real numbers which could be represented as column vectors . Thus if $psi_a$ and $\psi_b$ are in $\mathbb{R}^3$,
	\begin{eqnarray}\label{eqn:2.42-2.43}
		\psi_a &= \left(
		\begin{matrix}
			a1 \\ a2 \\ a3
		\end{matrix}
		\right)\\
		\psi_b &= \left(
		\begin{matrix}
			b1 \\b2 \\b3
		\end{matrix}
		\right)
	\end{eqnarray}
	where $a_i$ and $b_i$ are real. \\
	We could define the scalar product of $\psi_a$ and $\psi_b$ as
	\begin{eqnarray}\label{eqn:2.44-2.46}
		(\psi_a, \psi_b) &_\equiv^{def} 
		\left(
		\begin{matrix}
			a_1 & a_2 & a_3
		\end{matrix}
		\right)\left(
		\begin{matrix}
			b_1 \\ b_2 \\ b_3
		\end{matrix}
		\right) \\
		&= \psi_a^T \psi_b \\
		&- \sum_{i=1}^{3} a_i b_i
	\end{eqnarray}
	This scalar product also has all the postulated properties of a scalar product.\\
	
	In case of the vector space $\mathbb{R}^3$, the vectors $\psi_a$ and $\psi_b$ could be represented as directed lines $\vec{a}$ and $\vec{b}$ in a three dimensional coordinate system.\\
	
	\textbf{figure}
	\\
	The Scalar product $(\psi_a, \psi_b)$ is the usual dot product
	\begin{eqnarray}\label{eqn:2.47-2.48}
		\vec{a} . \vec{b} &= a_1 b_1 + a_2 b_2 + a_3 b_3 \\
		&= |\vec{a}||\vec{b}| \cos(\theta)
	\end{eqnarray}
	Where $|\vec{a}|$ and $|\vec{b}|$ are the magnitudes of the vectors $\vec{a}$ and $\vec{b}$ defined as
	\begin{eqnarray}\label{eqn:2.49-2.50}
		|\vec{a}| &\equiv \sqrt{(\psi_a , \psi_a)} = \sqrt{a_1^2 + a_2^2 + a_3^2} \\
		|\vec{b}| &\equiv \sqrt{(\psi_b, \psi_b)} = \sqrt{b_1^2 + b_2^2 + b_3^2}
	\end{eqnarray}
	
\end{enumerate}
\section{Norm of a vector}
If a vector space is enclosed with a scalar product, then the scalar product gives us the concept of the "magnitude" or "length" of a vector. In a general vector space the "magnitude" or "length" of a vector is called the norm of the vector. We simply define the norm of a vector $\psi_a$ as
\begin{equation}\label{eqn:2.51}
||\psi_a|| _\equiv^{def} \sqrt{(\psi_a, \psi_a)}
\end{equation}
The norm has the following properties:
\begin{enumerate}
	\item 
	\begin{equation}\label{eqn:2.52}
	||\psi_a|| \geq 0
	\end{equation}
	the equality holds only if the vector is null.
	
	\item 
	\begin{equation}\label{eqn:2.53}
	||\psi_a + \psi_b|| \leq ||\psi_a|| + ||\psi_b||
	\end{equation}
	This is called the triangle inequality
	
	\item 
	\begin{equation}\label{eqn:2.54}
	||\psi_a - \psi_b|| = ||\psi_b - \psi_a||
	\end{equation}
\end{enumerate}

\section{Metric included by the scalar product}
The norm included by the scalar product allows us to develop the concept of "distance" between vectors in a vector space. We say two vectors $\psi_a$ and $\psi_b$ are 'close' if $||\psi_a - \psi_b||$ is small. The metric in a vector space assigns a real number to the vector $\psi_a - \psi_b$. This real number is a measure of how close the two vectors are. We simply define the metric $\mathbf{d}(\psi_a, \psi_b)$ as
\begin{equation}\label{eqn:2.55}
\mathbf{d}(\psi_a, \psi_b) _\equiv^{def} ||\psi_a - \psi_b||
\end{equation}
Thus, if there are three vectors $\psi_a$, $\psi_b$ and $\psi_c$ and if $\mathbf{d}(\psi_a, \psi_b) < \mathbf{d}(\psi_a, \psi_c)$ then we say $\psi_a$ is closer to $\psi_b$ than to $\psi_c$.

\section{Schwarz's inequality}
We will now prove a very important inequality called Schwarz inequality which states
\begin{equation}\label{eqn:2.56}
|(\psi_a, \psi_b)| \leq \sqrt{(\psi_a, \psi_a)(\psi_b, \psi_b)}
\end{equation}
or
\begin{equation}\label{eqn:2.57}
|(\psi_a, \psi_b)| \leq ||\psi_a ||\ ||\psi_b||
\end{equation}
\textbf{proof : }
Let
\begin{equation}\label{eqn:2.58}
\psi = \psi_a + \lambda \psi_b
\end{equation}
Then
\begin{eqnarray}\label{eqn:2.59-2.60}
	(\psi, \psi) &= (\psi_a + \lambda \psi_b, \psi_a + \lambda \psi_b) \\
	&= (\psi_a, \psi_a) + \lambda (\psi_a, \psi_b) + \lambda^* (\psi_b, \psi_a) + |\lambda|^2 (\psi_b, \psi_b) \geq 0
\end{eqnarray}
The best inequality is obtained if $\lambda$ is chosen so as to minimize the left hand side of the above equation. By differentiation, the value of $\lambda$ which accomplishes this is found to be
\begin{equation}\label{eqn:2.61}
\lambda = - \frac{(\psi_b, \psi_a)}{\psi_b, \psi_b}
\end{equation}
Substituting this value of $\lambda$ in the above equation yields the Schwarz inequality.\\

We note that the equality sign holds if and only if $(\psi, \psi) = 0$, i.e., $\psi$ is the null vector, i.e., $\psi = \phi$, in other words
\begin{equation}\label{eqn:2.62}
\psi_a + \lambda \psi_b = \phi (null)
\end{equation}
i.e.,
\begin{eqnarray}\label{eqn:2.63-2.64}
	\psi_a &= - \lambda \psi_b + \phi \\
	\psi_a &= -\lambda \psi_b
\end{eqnarray}
Hence, the equality holds if $\psi_a$ and $\psi_b$ are multiple of each other, or if $\psi_a$ and $\psi_b$ are "parallel".\\
It follows from the Schwarz inequality that the scalar product $(\psi_a, \psi_b)$ is finite if the norms of $\psi_a$ and $\psi_b$ are finite.

\section{Analogy of Schwarz inequlity with vectors in a three-dimensional Euclidean space $\mathbb{R}^3$}
In $\mathbb{R}^3$, the vectors can be represented by directed lines (i.e. arrows). We have the scalar product ordinary vectors in the form
\begin{equation}\label{eqn:2.65}
\vec{A} . \vec{B} = |\vec{A}| \ |\vec{B}| \cos(\theta)
\end{equation}
Since cosine of any angle lies between $-1$ and $+1$, we have
\begin{equation}\label{eqn:2.66}
|\vec{A} . \vec{B} | \leq |\vec{A}| \ |\vec{B}|
\end{equation}
The analogue of this equation for a general vector space is the Schwarz inequality
\begin{equation}\label{eqn:2.67}
|(\psi_a, \psi_b)| \leq ||\psi_a|| \ ||\psi_b||
\end{equation}


\section{Orthogonality and linear independence}
A vector whose norm is unity is called a unit vector. For any given non-null vector, a unit vector can be formed by defining the vector by its norm. Thus
\begin{equation}\label{eqn:2.68}
u_a = \frac{\psi_a}{||\psi_a||}
\end{equation}
is normalized.\\
Two vectors $\psi_a$ and $\psi_b$ are orthogonal if their inverse product is zero, i.e., if
\begin{equation}\label{eqn:2.69}
(\psi_a, \psi_b ) = 0
\end{equation}
The unit vectors $u_1, u_2, \ldots, u_N$ form an orthogonal set if they are mutually orthonormal , i.e., 
\begin{equation}\label{eqn:2.70}
(u_i, u_j) = \delta_{ij}
\end{equation}

\subsection{Linear independence}
The set of vectors $\psi_1, \psi_2, \ldots, \psi_N$ are linearly independent if none of them can be expressed as a linear combination of the others. Mathematically this means that the equation
\begin{equation}\label{eqn:2.71}
\sum_{j=1}^{N} c_j \psi_j = 0
\end{equation}	
cannot be satisfied except by $c_j = 0$ for all $j$.

\section{Orthonormality and linear independence}
A dot of mutually orthogonal vectors (not necessarily normalized) are necessarily linearly independent. The converse is not true, however. That is, a set of linearly independent vectors may not be mutually orthogonal.\\
It  is always possible to orthonormalize a set of linearly independent vectors. By this we mean that from a given set of $N$ linearly independent vectors, it is possible to form a set of $N$ orthonormal vectors. This procedure is called \textbf{Schmidt orthonormalization method}.

\section{Schmidt orthonormalization method}
\label{chapter2.schmidt-orthonormalization-method}
Suppose $\psi_1, \psi_2, \ldots, \psi_N$ is a set of linearly independent vectors. Let
\begin{equation}\label{eqn:2.72}
u_1 = \frac{\psi_1}{||\psi_1||}
\end{equation}
Then $(u_1, u_1) = 1$, i.e., $u_1$ is normalized. Next construct the vector $\psi_2^\prime$ as follows:
\begin{equation}\label{eqn:2.73}
\psi_2^\prime = \psi_2 - u_1(u_1, \psi_2)
\end{equation}
i.e., to obtain $\psi_2^\prime$ we have subtracted the 'component' of $\psi_2$ along the $u_1$ "direction". Then it follows that
\begin{eqnarray}\label{eqn:2.74-2.76}
	(u_1, \psi_2^\prime) &= (u_1, \psi_2) - (u_1, u_1)(u_1, \psi_2) \\
	&= (u_1, \psi_2) - (u_1, \psi_2) \\
	&= 0
\end{eqnarray}
i.e., $\psi_2^\prime$ is orthogonal to $u_1$. We then normalize $\psi_2^\prime$, i.e.,
\begin{equation}\label{eqn:2.77}
u_2 = \frac{\psi_2^\prime}{||\psi_2^\prime ||}
\end{equation}
We can continue the process until we exhaust all the vectors. For example, in the next step we can write
\begin{equation}\label{eqn:2.78}
\psi_3^\prime = \psi_3 - u_1 (u_1, \psi_3) - u_2 (u_2, \psi_3)
\end{equation}
We note immediately that $\psi_3^\prime$ is orthogonal to both $u_1$ and $u_2$, i.e.,
\begin{equation}\label{eqn:2.79}
(u_1, \psi_3^\prime) = (u_2, \psi_3^\prime) = 0
\end{equation}
We normalize $\psi_3^\prime$ to get $u_3$, i.e.,
\begin{equation}\label{eqn:2.80}
u_3 = \frac{\psi_3^\prime}{||\psi_3^\prime ||}
\end{equation}
Finally, in the $N-th$ step, we write
\begin{equation}\label{eqn:2.81}
\psi_N^\prime = \psi_N - u_1(u_1, \psi_N) - u_2(u_2, \psi_N) - \ldots - u_{N-1}(u_{N-1}, \psi_N)
\end{equation}
$\psi_N^\prime$ is orthogonal to $u_1, u_2, \ldots, u_{N-1}$, i.e.,
\begin{equation}\label{eqn:2.82}
(u_1, \psi_N^\prime) = (u_2, \psi_N^\prime) = \ldots = (u_{N-1}, \psi_N^\prime) = 0
\end{equation}
Normalizing $\psi_N^\prime$ we get
\begin{equation}\label{eqn:2.83}
u_N = \frac{\psi_N^\prime}{||\psi_N^\prime ||}
\end{equation}
Thus, the set $\{u_1, u_2, \ldots, u_N\}$ is an orhtonormal set of vectors.

\section{Dimension of a vector space}
The vector space $V$ is said to be n-dimensional if there exists $n$ linearly independent vectors, but if $n+1$ vectors are linearly dependent the dimension may be finite or infinite.

\section{Complete vector space}
Before defining what a complete vector space is we will give some other definitions.\\
A sequence of vectors $\{\psi_n\}$ in the vector space $V$ is called a \textbf{Cauchy sequence} if for every $\epsilon > 0$ there exists an integer $N$ such that
\begin{equation}\label{eqn:2.84}
||\psi_n - \psi_m || < \epsilon
\end{equation}
if $n, m > N$. In other words, the vectors in the sequence come 'closer' if the index increases. In particular
\begin{equation}\label{eqn:2.85}
||\psi_n - \psi_m || \rightarrow 0 \ as \ n,m \rightarrow \infty
\end{equation}

\section{Convergence of a sequence of vectors in a vector space}
A sequence $\{\psi_n\}$ , $n = 1, 2, \ldots$ in a vector space $V$ converses to a vector $\psi$ in $V$ if for every $\epsilon > 0$ there exists an integer $N$ such that
\begin{equation}\label{eqn:2.86}
||\psi_n - \psi_m || < \epsilon
\end{equation}
if $n > N$, that is if
\begin{equation}\label{eqn:2.87}
\lim\limits_{n\rightarrow \infty} ||\psi - \psi_n|| = 0
\end{equation}
then 
\begin{equation}\label{eqn:2.88}
\{\psi_n \} \rightarrow \psi
\end{equation}
and the sequence is called a convergent sequence.\\
Now we can show every convergent sequence is a Cauchy sequence.\\
\textbf{proof}\\
Let $\{\psi_n \} \rightarrow \psi$. Then
\begin{eqnarray}\label{eqn:2.89-2.90}
	|| \psi_n - \psi_m || &= ||\psi_n - \psi + \psi -\psi_m|| \\
	&\leq ||\psi_n - \psi|| + ||\psi -\psi_m||
\end{eqnarray}
here the triangle inequality is used.\\
Since $\{\psi_n \}$ is a convergent sequence, each term on the right hand side tends to zero as $n$ and $m$ tends to infinity. Hence $||\psi_n - \psi_m|| \rightarrow 0$ as $n,m=\infty$, i.e., the sequence $\{\psi_i\}$ with $i=1,2,\ldots$ is a Cauchy sequence.\\
The converse of the above statement is not true in general. In other words, \textit{a Cauchy sequence in a vector space may not converge to a vector in the space}. It can be shown that for a finite dimensional vector space the conver is true, i.e., in a finite dimensional vector space a Cauchy sequence is always a convergent sequence. Exceptions may arise in infinite-dimensional vector space.

\subsection{An example of a vector space whose Cauchy sequence does not converse to a vector in the vector space}
Consider the vector space consisting of all continuous function of a single real variable $x$ in the range $[-1, 1]$. In this vector space consider a sequence $\{f_k (x) \}$, $k = 1,2,\ldots$ of the following form:
\begin{equation}\label{eqn:2.91}
f_k(x) = \begin{cases}
1 \ \text{ for } \frac{1}{k} \leq x \leq 1 \\
\frac{k x + 1}{2} \ \text{ for } \ -\frac{1}{k} < x < \frac{1}{k} \\
0 \ \text{ for } \ -1 \leq x \leq - \frac{1}{k}
\end{cases}
\end{equation}
$k = 1, 2, 3, \ldots$\\
The graph of the sequence of functions is shown below:
\\	
% figure
\textbf{figure}
\\
Note that,  in this example each $f_k(x)$ is continuous, but their first derivatives are discontinuous.\\

Let us define the scalar product in this space as
\begin{equation}\label{eqn:2.92}
(f, g) = \int_{-1}^{1} f^*(x) g(x) dx
\end{equation}
So that the metric $\mathbf{d}(f, g)$, i.e., the "distance" between vectors $f$ and $g$ can be defined as
\begin{eqnarray}\label{eqn:2.93-2.94}
	\mathbf{d}(f, g) &\equiv ||f-g|| \\
	&= \sqrt{\int_{-1}^{+1}\left(f^*(x) - g^*(x)\right)\left(f(x) - g(x)\right) dx}
\end{eqnarray}
With this metric we can show that the sequence ${f_k}$ defined above is indeed a Cauchy sequence. However, looking at the graph above, we see that as $k$ becomes large, $f_k$ approaches the $\theta$ function
\begin{equation}\label{eqn:2.95}
\theta(x) = 
\begin{cases}
0 \ \text{ for } \ x < 0 \\
1 \ \text{ for } \ x > 0
\end{cases}
\end{equation}
Which is a discontinuous function at $x=0$.\\
We show that graph of $\theta(x)$ below:
\\
%figure
\textbf{figure}
\\
Thus the Cauchy sequence $\{f_k(x) \}$ of continuous function is converging to a discontinuous function which lies outside the vector space $V$.\\
If, instead of chooding all continuous function, we had chosen all square integrable function as defining the vector space, then any Cauchy sequence in the vector space would converge to a vector in the space.\\
\textbf{Definition : } A linear vector space is said to be \underline{complete} if any Cauchy sequence converges to a vector in the space.

\section{Hilbert space}
A complete linear vector space, finite or infinite dimensional, ( and hence \textbf{@@@enclosed@@@} with  a norm and metric induced by the scalar product), is called a Hilbert space.\\
A finite-dimensional vector space is always complete. So, a finite dimensional linear vector space in which a scalar product is defined is a Hilbert space.\\
An infinite dimensional vector space with a scalar product may or may not be complete. Whether or not an infinite dimensional vector space is complete depends upon how exactly the vector space is defined and on the metric.

\section{Basis vectors in a Hilbert space}
\subsection{Finite dimensional space}
In a finite dimensional vector space of dimension $n$, any set of linearly independent vectors $\psi_1, \psi_2, \ldots, \psi_n$ spans the entire space. In other words any vector $\psi$ in the space can be expressed as linear combination of $\psi_1, \psi_2, \ldots, \psi_n$, i.e.,
\begin{equation}\label{eqn:2.96}
\psi = \sum_{i=1}^{n} a_i \psi_i
\end{equation}
The vectors $\psi_1, \psi_2, \ldots, \psi_n$ form a complete basis for the vector space. The vectors $\psi_1, \psi_2, \ldots, \psi_n$, even if linearly independent, may not be orthogonal to each other. It is more convenient to use a set of orthonormal vectors $\phi_1, \psi_2, \ldots, \phi_n$ as the basis. Being orthogonal, the vectors $\phi_1, \psi_2, \ldots, \phi_n$ are automatically linearly independent. The orthonormal set of basis vectors $\{\phi_i \}$ $i = 1,2,\ldots,n$ can be constructed from the set $\{\psi_i \}$ $i = 1,2,\ldots,n$ by using the schmidt orthonormalization procedure.\\
Choosing the orthonormal set as the basis, any vector $\psi$ in the vector space can be written as
\begin{equation}\label{eqn:2.97}
\psi = \sum_{i=1}^{n} a_i \phi_i
\end{equation}
where
\begin{equation}\label{eqn:2.98}
(\phi_i, \phi_i) = \delta_{ij}
\end{equation}
using equation \ref{eqn:2.98} we have
\begin{equation}\label{eqn:2.99}
a_i = (\phi_i, \psi)
\end{equation}
\subsection{Infinite dimensional vector space}
In an infinite dimensional vector space the number of basis vectors is infinity. Let $\{\phi_1, \phi_2, \ldots \}$ be an infinite set of orthonormal basis vectors spanning the infinite dimensional Hilbert space. This set of basis vectors is said to be \underline{complete} if \underline{any} vector $\psi$ in the Hilbert space  can be expanded as a linear combination of the basis vectors, i.e.,
\begin{equation}\label{eqn:2.100}
\psi = \sum_{i=1}^{\infty} a_i \phi_i
\end{equation}
In an infinite dimensional vector space, choosing an infinite number of basis vectors may not ensure that the basis set is complete. It may so happen that there are other linearly independent vectors, may be infinite in numbers, which have been missed in the first choice of the basis vectors.\\
Whenever we have an infinite sum, as in equation \ref{eqn:2.100} is to be understood in the sense that the sequences consisting of the partial sums
\begin{equation}\label{eqn:2.101}
f_n = \sum_{i=1}^{n} a_i \phi_i
\end{equation}
$n = 1,2,3,\ldots$ \\
converges to $\psi$, i.e.,
\begin{equation}\label{eqn:2.102}
\lim\limits_{n \rightarrow \infty} ||\psi - f_n || \rightarrow 0
\end{equation}
Since the vector $\psi$ must have a finite norm, we must have
\begin{equation}\label{eqn:2.103}
||\psi ||^2 = (\psi, \psi) = \sum_{i=1}^{\infty} |a_i|^2 < \infty \ \text{(finite)}
\end{equation}
If the basis vectors $\{\phi_i \}$ are orthonormal, we have
\begin{equation}\label{eqn:2.104}
a_i = (\phi_i, \psi)
\end{equation}
So that equation \ref{eqn:2.103} can be written as
\begin{equation}
\sum_{i=1}^{\infty} |(\phi_i, \psi)|^2 < \infty
\end{equation}
The scalar $a_i$ can be regarded as the components of $\psi$ in the 'directions' of $\phi_i$

\subsection{Example}
Question : Show that the set of all square integrable functions, i.e., set of all functions $f$ such that
\begin{equation}
\int_{-\infty}^{\infty} f^*(x) f(x) dx < \infty \ \text{(i.e., finite)}
\end{equation}
belong o a Hilbert space. This Hilbert space is denoted as $L^2(-\infty, \infty)$.\\
To show this, consider the following
\begin{enumerate}
	\item 
	If $f$ and $g$ are square integrable functions, so  is $f+g$, and hence $f+g$ also belongs to the Hilbert space
	\begin{equation}
	||f+g|| \leq ||f|| + ||g||
	\end{equation}
	
	\item 
	We can define the scalar product between $f$ and $g$ as follows:
	\begin{equation}
	(f, g) _\equiv^{def} \int_{-\infty}^{\infty} f^*(x) g(x) dx
	\end{equation}
	That the scalar product exists follows from the Schwarz inequality
	\begin{equation}
	|(f, g)| \leq ||f|| . ||g||, \ < \infty
	\end{equation}
	
	
	\item 
	It can also be shown that any Cauchy sequence of square integrable functions converges to a limit which is also square integrable. In other words, the space of all square integrable functions is complete.
\end{enumerate}
Hence the linear vector space consisting of all square integrable functions is indeed a Hilbert space.

\section{Dirac notation}
(Cohen-Tannoudji; page 109) \\
"ket" vectors and "bra" vectors.\\

\textbf{Notation} \\
Any element, or vector of a vector space $V$ is called a \underline{ket vector}, or more simply, a \underline{ket}. It is represented by the symbol $\ket{}$, inside which is placed a distinctive sign which enables us to distinguish between different kets, for example $\ket{\psi}$.
\subsection{Scalar product}
With each pair of kets $\ket{\phi}$ and $\ket{\psi}$, taken in this order, we associate a complex number, which is their scalar product $(\ket{\phi}, \ket{\psi})$ and which satisfies various properties discussed earlier (section \ref{inner_product}).

\subsection{Dual vector space}
Linear functional: A linear functional $\chi$ is a linear operation on the kets such that $\chi$ operating on a ket $\ket{\psi}$ gives a complex scalar:
\begin{equation}
\chi \ket{\psi} \rightarrow \ scalar \text{ where } \ \ket{\psi} \in V
\end{equation}
and
\begin{equation}
\chi (\lambda_1 \ket{\psi} + \lambda_2 \ket{\psi_2}) = \lambda_1 \chi \ket{\psi_1} + \lambda_2 \chi \ket{\psi_2}
\end{equation}
The set of all linear functions defined in the kets of a cector space $V$ themselves form a linear vector space called the dual space of $V$ and symbolized by $V^*$.

\subsection{Bra notation for the vectors of $V^*$}
Any element, or vector, of the space $V^*$ is called a "bra vector", or more simply, a bra. It is symbolized by $\bra{}$. For example, the bra $\bra{\chi}$ designates the inear functional $\chi$ we shall henceforth use the notation $\braket{\chi}{\psi}$ to denote the number obtained by causing the linear funcitonal $\bra{\chi} \in V^*$ to act on the ket $\ket{\psi} \in V$. Thus
\begin{equation}
\chi (\ket{\psi}) = \braket{\chi}{\psi}
\end{equation}

\subsection{Correspondence between kets and bras}
The existence of the scalar product in $V$ will now enable us to show that we can associate with every ket $\ket{\phi} \in V$ and element of $V^*$, that in a bra, which will be denoted by $\bra{\phi}$.\\
The ket $\ket{\phi}$ does indeed enables us to define a linear functional, the one which associates with each $\ket{\psi} \in V$ a complex number which is equal to the scalar product $(\ket{\phi} , \ket{\psi})$. Let $\bra{\phi}$ be this linear functional. It is thus defined by the relation
\begin{equation}
\braket{\phi}{\psi} = ( \ket{\phi}, \ket{\psi} )
\end{equation}

\subsection{The correspondence in anti-linear}
Let $\lambda_1\ket{\phi_1} + \lambda_2\ket{\phi_2}$  be a ket. Then
\begin{eqnarray}
	(\lambda_1\ket{\phi_1} + \lambda_2\ket{\phi_2}, \psi) 
	&= \lambda_1^* (\ket{\phi_1}, \ket{\psi}) + \lambda_2^* (\ket{\phi_2}, \ket{\psi}) \\
	&= \lambda_1^* \braket{\phi_1}{\psi} + \lambda_2^* \braket{\phi_2}{\psi}\\
	&= (\lambda_1^* \bra{\phi_1} + \lambda_2^* \bra{\phi_2}) \ket{\psi}
\end{eqnarray}
Thus
\begin{equation}
\lambda_1\ket{\phi_1} + \lambda_2 \ket{\phi_2} _\rightarrow^{dc} \lambda_1^* \bra{\phi_1} + \lambda_2^* \bra{\phi_2}
\end{equation}
Where "dc" is short for dual correspondence.\\
\textbf{comment}\\
If $\lambda$ is a complex number and $\ket{\psi}$ is a ket, then $\lambda\ket{\psi}$ is also a ket. We are sometimes led to write $\lambda\ket{\psi}$ as $\ket{\lambda\psi}$:
\begin{equation}
\ket{\lambda\psi} = \lambda \ket{\psi}
\end{equation}
One must be careful to remember that $\bra{\lambda\psi}$ represents the bra associated with the ket $\ket{\lambda\psi}$. since the correspondence between a bra and a ket is anti-linear we have
\begin{equation}
\bra{\lambda\psi} = \lambda^*\bra{\psi}
\end{equation}
\subsection{Dirac notation for the scalar product}
We now have at our disposal two distinct notations for designating the scalar product of $\ket{\psi}$ by $\ket{\phi}$, namely, $(\ket{phi}, \ket{\psi})$ and $\braket{\phi}{\psi}$, $\bra{\phi}$ being the bra associated with the ket $\ket{\phi}$. We shall mostly use the Dirac notation $\braket{\phi}{\psi}$. In the table below we summarize, in Dirac notation, the properties of the scalar product.
\begin{eqnarray}
	\braket{\phi}{\psi} &= \braket{\psi}{\phi}^* \\
	\braket{\phi}{\lambda_1\psi_1 + \lambda_2\psi_2} &= \lambda_1 \braket{\phi}{\psi_1} + \lambda_2 \braket{\phi}{\psi_2} \\
	\braket{\lambda_1\phi_1 + \lambda_2\phi_2}{\psi} &= \lambda_1^* \braket{\phi_1}{\psi} + \lambda_2^* \braket{\phi_2}{\psi} \\
\end{eqnarray}
$\braket{\psi}{\psi}$ is real, positive; zero if and only if $\ket{\psi} = \phi$ (null)



